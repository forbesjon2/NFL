{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70409bb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "set_printoptions() got an unexpected keyword argument 'suppress'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Set device to GPU if available \u001b[39;00m\n\u001b[1;32m     39\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_printoptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuppress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: set_printoptions() got an unexpected keyword argument 'suppress'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from NFLUtils import NFLUtils\n",
    "nfl_utils = NFLUtils()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# ANN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import logging\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# Set pandas display option to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# XGBoost \n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import seaborn as sns # confusion matrix\n",
    "\n",
    "# Set device to GPU if available \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "556dd56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff2364",
   "metadata": {},
   "source": [
    "### Load CSV & UMAP model\n",
    "cp Combined.csv ~/drive/Notes/ML/Pytorch/footballData/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfebbf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5464 entries, 0 to 5463\n",
      "Data columns (total 70 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Unnamed: 0               5464 non-null   int64  \n",
      " 1   Season                   5464 non-null   int64  \n",
      " 2   Date                     5464 non-null   object \n",
      " 3   Home_Team                5464 non-null   object \n",
      " 4   Visitor_Team             5464 non-null   object \n",
      " 5   H_halftime_odds          5464 non-null   float64\n",
      " 6   V_halftime_odds          5464 non-null   float64\n",
      " 7   H_start_odds             5464 non-null   float64\n",
      " 8   V_start_odds             5464 non-null   float64\n",
      " 9   H_Won                    5464 non-null   float64\n",
      " 10  D_First_Downs            5464 non-null   float64\n",
      " 11  D_Rush                   5464 non-null   float64\n",
      " 12  D_Yds                    5464 non-null   float64\n",
      " 13  D_TDs                    5464 non-null   float64\n",
      " 14  D_Cmp                    5464 non-null   float64\n",
      " 15  D_Att                    5464 non-null   float64\n",
      " 16  D_Yd                     5464 non-null   float64\n",
      " 17  D_TD                     5464 non-null   float64\n",
      " 18  D_INT                    5464 non-null   float64\n",
      " 19  D_Sacked                 5464 non-null   float64\n",
      " 20  D_Sacked_Yards           5464 non-null   float64\n",
      " 21  D_Net_Pass_Yards         5464 non-null   float64\n",
      " 22  D_Total_Yards            5464 non-null   float64\n",
      " 23  D_Fumbles                5464 non-null   float64\n",
      " 24  D_Lost                   5464 non-null   float64\n",
      " 25  D_Turnovers              5464 non-null   float64\n",
      " 26  D_Penalties              5464 non-null   float64\n",
      " 27  D_Third_Down_Conv        5464 non-null   float64\n",
      " 28  D_Fourth_Down_Conv       5464 non-null   float64\n",
      " 29  D_Time_of_Possession     5464 non-null   float64\n",
      " 30  D_passing_att            5464 non-null   float64\n",
      " 31  D_passing_cmp            5464 non-null   float64\n",
      " 32  D_passing_int            5464 non-null   float64\n",
      " 33  D_passing_lng            5464 non-null   float64\n",
      " 34  D_passing_sk             5464 non-null   float64\n",
      " 35  D_passing_td             5464 non-null   float64\n",
      " 36  D_receiving_lng          5464 non-null   float64\n",
      " 37  D_rushing_att            5464 non-null   float64\n",
      " 38  D_rushing_lng            5464 non-null   float64\n",
      " 39  D_rushing_td             5464 non-null   float64\n",
      " 40  D_rushing_yds            5464 non-null   float64\n",
      " 41  D_passing_rushing_td     5464 non-null   float64\n",
      " 42  D_def_interceptions_int  5464 non-null   float64\n",
      " 43  D_def_interceptions_td   5464 non-null   float64\n",
      " 44  D_def_interceptions_yds  5464 non-null   float64\n",
      " 45  D_fumbles_ff             5464 non-null   float64\n",
      " 46  D_fumbles_fr             5464 non-null   float64\n",
      " 47  D_fumbles_td             5464 non-null   float64\n",
      " 48  D_fumbles_yds            5464 non-null   float64\n",
      " 49  D_sk                     5464 non-null   float64\n",
      " 50  D_tackles_ast            5464 non-null   float64\n",
      " 51  D_tackles_comb           5464 non-null   float64\n",
      " 52  D_tackles_solo           5464 non-null   float64\n",
      " 53  D_kick_punt_returns_lng  5464 non-null   float64\n",
      " 54  D_kick_punt_returns_rt   5464 non-null   float64\n",
      " 55  D_kick_punt_returns_yds  5464 non-null   float64\n",
      " 56  D_punting_pnt            5464 non-null   float64\n",
      " 57  D_punting_avg            5464 non-null   float64\n",
      " 58  D_scoring_fga            5464 non-null   float64\n",
      " 59  D_scoring_fgp            5464 non-null   float64\n",
      " 60  D_scoring_xpa            5464 non-null   float64\n",
      " 61  D_scoring_xpp            5464 non-null   float64\n",
      " 62  D_Final                  5464 non-null   float64\n",
      " 63  D_Final_Allowed          5464 non-null   float64\n",
      " 64  D_start_odds             5464 non-null   float64\n",
      " 65  D_halftime_odds          5464 non-null   float64\n",
      " 66  D_datediff               5464 non-null   float64\n",
      " 67  D_pythagorean            5464 non-null   float64\n",
      " 68  kick_punt_umap_dim_1     5264 non-null   float64\n",
      " 69  kick_punt_umap_dim_2     5264 non-null   float64\n",
      "dtypes: float64(65), int64(2), object(3)\n",
      "memory usage: 2.9+ MB\n",
      "df after perf set removed: (5264, 70)\n",
      "performance set size: (200, 70)\n",
      "df after missing odds removed: (5264, 70)\n",
      "df perf after missing odds removed: (191, 70)\n",
      "<class 'umap.umap_.UMAP'>\n"
     ]
    }
   ],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(\"./footballData/CombinedSlidingWindow4.csv\", index_col=False, low_memory=False)\n",
    "\n",
    "df.info()\n",
    "\n",
    "# The performance set's size is defined in the SlidingWindowNFL-1 file. When kick_punt_umap_dim_1 (or 2) is blank\n",
    "test_performance_df = df[df['kick_punt_umap_dim_1'].isna()]\n",
    "df = df[df['kick_punt_umap_dim_1'].isna() == False]\n",
    "print(f'df after perf set removed: {df.shape}')\n",
    "print(f'performance set size: {test_performance_df.shape}')\n",
    "\n",
    "# Remove missing odds data (Ignore data with no odds?)\n",
    "test_performance_df = test_performance_df[test_performance_df['D_start_odds'] != 0.0]\n",
    "\n",
    "print(f'df after missing odds removed: {df.shape}')\n",
    "print(f'df perf after missing odds removed: {test_performance_df.shape}')\n",
    "\n",
    "# Load the UMAP\n",
    "filename = \"kick_punt_umap.sav\"\n",
    "umap_model = None\n",
    "try:\n",
    "    with open(filename, 'rb') as file:\n",
    "        umap_model = pickle.load(file)\n",
    "        print(type(umap_model))\n",
    "except EOFError:\n",
    "    print(\"The file is empty or corrupt. Please check its content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26c9e0",
   "metadata": {},
   "source": [
    "### Remove items w/ missing odds data, apply UMAP to performance set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593aec0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 2)\n"
     ]
    }
   ],
   "source": [
    "# Remove missing odds data\n",
    "test_performance_df = test_performance_df[test_performance_df['D_start_odds'] != 0.0]\n",
    "print(test_performance_df.shape)\n",
    "\n",
    "# ---- Apply UMAP to performance set ----\n",
    "# Fit standardScaler on the training set\n",
    "umap_columns = [\"D_kick_punt_returns_lng\", \"D_kick_punt_returns_rt\", \"D_kick_punt_returns_yds\"]\n",
    "umap_train_df = df[umap_columns]\n",
    "umap_scaler = StandardScaler().fit(umap_train_df)\n",
    "\n",
    "# Scale the test set\n",
    "scaled_return_game_df = umap_scaler.transform(test_performance_df[umap_columns])\n",
    "\n",
    "if umap_model is None:\n",
    "    print(\"UMAP not correctly loaded FIX NOW\")\n",
    "\n",
    "umap_embedding = umap_model.transform(scaled_return_game_df)\n",
    "print(umap_embedding.shape)\n",
    "\n",
    "# Create the two new columns, drop the 4\n",
    "test_performance_df['kick_punt_umap_dim_1'] = umap_embedding[:,0]\n",
    "test_performance_df['kick_punt_umap_dim_2'] = umap_embedding[:,1]\n",
    "\n",
    "test_performance_df.drop(umap_columns, axis=1, inplace=True)\n",
    "df.drop(umap_columns, axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd9bfcf9-0964-4ed7-8fcb-3512b324b220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Season</th>\n",
       "      <th>Date</th>\n",
       "      <th>Home_Team</th>\n",
       "      <th>Visitor_Team</th>\n",
       "      <th>H_halftime_odds</th>\n",
       "      <th>V_halftime_odds</th>\n",
       "      <th>H_start_odds</th>\n",
       "      <th>V_start_odds</th>\n",
       "      <th>H_Won</th>\n",
       "      <th>D_First_Downs</th>\n",
       "      <th>D_Rush</th>\n",
       "      <th>D_Yds</th>\n",
       "      <th>D_TDs</th>\n",
       "      <th>D_Cmp</th>\n",
       "      <th>D_Att</th>\n",
       "      <th>D_Yd</th>\n",
       "      <th>D_TD</th>\n",
       "      <th>D_INT</th>\n",
       "      <th>D_Sacked</th>\n",
       "      <th>D_Sacked_Yards</th>\n",
       "      <th>D_Net_Pass_Yards</th>\n",
       "      <th>D_Total_Yards</th>\n",
       "      <th>D_Fumbles</th>\n",
       "      <th>D_Lost</th>\n",
       "      <th>D_Turnovers</th>\n",
       "      <th>D_Penalties</th>\n",
       "      <th>D_Third_Down_Conv</th>\n",
       "      <th>D_Fourth_Down_Conv</th>\n",
       "      <th>D_Time_of_Possession</th>\n",
       "      <th>D_passing_att</th>\n",
       "      <th>D_passing_cmp</th>\n",
       "      <th>D_passing_int</th>\n",
       "      <th>D_passing_lng</th>\n",
       "      <th>D_passing_sk</th>\n",
       "      <th>D_passing_td</th>\n",
       "      <th>D_receiving_lng</th>\n",
       "      <th>D_rushing_att</th>\n",
       "      <th>D_rushing_lng</th>\n",
       "      <th>D_rushing_td</th>\n",
       "      <th>D_rushing_yds</th>\n",
       "      <th>D_passing_rushing_td</th>\n",
       "      <th>D_def_interceptions_int</th>\n",
       "      <th>D_def_interceptions_td</th>\n",
       "      <th>D_def_interceptions_yds</th>\n",
       "      <th>D_fumbles_ff</th>\n",
       "      <th>D_fumbles_fr</th>\n",
       "      <th>D_fumbles_td</th>\n",
       "      <th>D_fumbles_yds</th>\n",
       "      <th>D_sk</th>\n",
       "      <th>D_tackles_ast</th>\n",
       "      <th>D_tackles_comb</th>\n",
       "      <th>D_tackles_solo</th>\n",
       "      <th>D_punting_pnt</th>\n",
       "      <th>D_punting_avg</th>\n",
       "      <th>D_scoring_fga</th>\n",
       "      <th>D_scoring_fgp</th>\n",
       "      <th>D_scoring_xpa</th>\n",
       "      <th>D_scoring_xpp</th>\n",
       "      <th>D_Final</th>\n",
       "      <th>D_Final_Allowed</th>\n",
       "      <th>D_start_odds</th>\n",
       "      <th>D_halftime_odds</th>\n",
       "      <th>D_datediff</th>\n",
       "      <th>D_pythagorean</th>\n",
       "      <th>kick_punt_umap_dim_1</th>\n",
       "      <th>kick_punt_umap_dim_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5264</th>\n",
       "      <td>7449</td>\n",
       "      <td>2023</td>\n",
       "      <td>2024-01-21</td>\n",
       "      <td>KAN</td>\n",
       "      <td>BUF</td>\n",
       "      <td>2.386667</td>\n",
       "      <td>1.577778</td>\n",
       "      <td>2.073398</td>\n",
       "      <td>1.724771</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.049</td>\n",
       "      <td>14.342</td>\n",
       "      <td>-4.790</td>\n",
       "      <td>-10.070</td>\n",
       "      <td>-1.931</td>\n",
       "      <td>-67.962</td>\n",
       "      <td>5.714</td>\n",
       "      <td>52.173</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>3.239</td>\n",
       "      <td>1.041</td>\n",
       "      <td>-18.251</td>\n",
       "      <td>-53.359</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>1.029</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-3.945</td>\n",
       "      <td>-13.555</td>\n",
       "      <td>27.766</td>\n",
       "      <td>-4.679</td>\n",
       "      <td>3.271</td>\n",
       "      <td>1.649</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>13.745</td>\n",
       "      <td>-10.854</td>\n",
       "      <td>-4.265</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-35.108</td>\n",
       "      <td>-1.125</td>\n",
       "      <td>-1.165</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-8.337</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.277</td>\n",
       "      <td>25.928</td>\n",
       "      <td>0.859</td>\n",
       "      <td>-2.725</td>\n",
       "      <td>5.923</td>\n",
       "      <td>8.648</td>\n",
       "      <td>0.551</td>\n",
       "      <td>2.611</td>\n",
       "      <td>1.207</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-1.272</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.809</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.944908</td>\n",
       "      <td>9.706598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5265</th>\n",
       "      <td>7444</td>\n",
       "      <td>2023</td>\n",
       "      <td>2024-01-28</td>\n",
       "      <td>DET</td>\n",
       "      <td>SFO</td>\n",
       "      <td>1.263333</td>\n",
       "      <td>3.894444</td>\n",
       "      <td>4.307745</td>\n",
       "      <td>1.203585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-5.287</td>\n",
       "      <td>1.664</td>\n",
       "      <td>-19.688</td>\n",
       "      <td>6.013</td>\n",
       "      <td>-10.797</td>\n",
       "      <td>3.569</td>\n",
       "      <td>26.740</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>4.852</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>15.220</td>\n",
       "      <td>-15.201</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>16.338</td>\n",
       "      <td>-14.753</td>\n",
       "      <td>38.606</td>\n",
       "      <td>-1.158</td>\n",
       "      <td>2.920</td>\n",
       "      <td>3.124</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-4.086</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>4.240</td>\n",
       "      <td>-1.072</td>\n",
       "      <td>-18.576</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-30.421</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.961</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-2.540</td>\n",
       "      <td>-1.567</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.897</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>2.681</td>\n",
       "      <td>3.104</td>\n",
       "      <td>-2.631</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>4.765425</td>\n",
       "      <td>5.378789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5266</th>\n",
       "      <td>7445</td>\n",
       "      <td>2023</td>\n",
       "      <td>2024-01-28</td>\n",
       "      <td>KAN</td>\n",
       "      <td>BAL</td>\n",
       "      <td>1.345556</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>2.885004</td>\n",
       "      <td>1.396731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.149</td>\n",
       "      <td>-29.932</td>\n",
       "      <td>-1.342</td>\n",
       "      <td>3.770</td>\n",
       "      <td>2.065</td>\n",
       "      <td>5.677</td>\n",
       "      <td>3.682</td>\n",
       "      <td>1.174</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-6.887</td>\n",
       "      <td>15.044</td>\n",
       "      <td>-17.795</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.246</td>\n",
       "      <td>1.906</td>\n",
       "      <td>-1.403</td>\n",
       "      <td>-7.241</td>\n",
       "      <td>0.512</td>\n",
       "      <td>7.909</td>\n",
       "      <td>4.222</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-2.455</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-1.230</td>\n",
       "      <td>-16.767</td>\n",
       "      <td>-3.475</td>\n",
       "      <td>-19.850</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-32.839</td>\n",
       "      <td>-1.727</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-13.352</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.207</td>\n",
       "      <td>14.747</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-8.664</td>\n",
       "      <td>-8.438</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>1.400</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-1.516</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-8.754</td>\n",
       "      <td>-2.969</td>\n",
       "      <td>1.488</td>\n",
       "      <td>-1.877</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.746289</td>\n",
       "      <td>10.155971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5267</th>\n",
       "      <td>7645</td>\n",
       "      <td>2023</td>\n",
       "      <td>2024-02-11</td>\n",
       "      <td>SFO</td>\n",
       "      <td>KAN</td>\n",
       "      <td>1.301111</td>\n",
       "      <td>3.472222</td>\n",
       "      <td>1.757009</td>\n",
       "      <td>2.028809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.930</td>\n",
       "      <td>23.153</td>\n",
       "      <td>3.614</td>\n",
       "      <td>-38.602</td>\n",
       "      <td>1.845</td>\n",
       "      <td>-27.846</td>\n",
       "      <td>2.388</td>\n",
       "      <td>45.530</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-3.815</td>\n",
       "      <td>6.219</td>\n",
       "      <td>22.362</td>\n",
       "      <td>34.943</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>-6.133</td>\n",
       "      <td>18.912</td>\n",
       "      <td>-20.599</td>\n",
       "      <td>2.204</td>\n",
       "      <td>-1.002</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.194</td>\n",
       "      <td>6.831</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.370</td>\n",
       "      <td>3.113</td>\n",
       "      <td>1.567</td>\n",
       "      <td>2.206</td>\n",
       "      <td>0.951</td>\n",
       "      <td>12.581</td>\n",
       "      <td>1.320</td>\n",
       "      <td>1.064</td>\n",
       "      <td>0.080</td>\n",
       "      <td>13.771</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.668</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-14.084</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>-2.174</td>\n",
       "      <td>-5.335</td>\n",
       "      <td>-3.160</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-1.046</td>\n",
       "      <td>-1.315</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>1.278</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>3.839</td>\n",
       "      <td>3.252</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-2.171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104</td>\n",
       "      <td>6.982881</td>\n",
       "      <td>4.331357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5268</th>\n",
       "      <td>7808</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-10-10</td>\n",
       "      <td>SFO</td>\n",
       "      <td>SEA</td>\n",
       "      <td>1.056667</td>\n",
       "      <td>9.383333</td>\n",
       "      <td>1.461897</td>\n",
       "      <td>2.642566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.140</td>\n",
       "      <td>14.184</td>\n",
       "      <td>7.482</td>\n",
       "      <td>-32.831</td>\n",
       "      <td>-7.632</td>\n",
       "      <td>-101.676</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>56.893</td>\n",
       "      <td>-1.129</td>\n",
       "      <td>-1.901</td>\n",
       "      <td>-3.625</td>\n",
       "      <td>-37.717</td>\n",
       "      <td>-7.982</td>\n",
       "      <td>1.507</td>\n",
       "      <td>0.691</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-21.287</td>\n",
       "      <td>6.148</td>\n",
       "      <td>1.838</td>\n",
       "      <td>0.993</td>\n",
       "      <td>-15.357</td>\n",
       "      <td>-12.401</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>-3.408</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>0.452</td>\n",
       "      <td>-3.691</td>\n",
       "      <td>10.496</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-1.471</td>\n",
       "      <td>29.735</td>\n",
       "      <td>-1.018</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.460</td>\n",
       "      <td>24.419</td>\n",
       "      <td>1.353</td>\n",
       "      <td>2.059</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>0.618</td>\n",
       "      <td>-1.915</td>\n",
       "      <td>-3.603</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>-1.445</td>\n",
       "      <td>-0.773</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-6.312</td>\n",
       "      <td>-1.181</td>\n",
       "      <td>-8.327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-2.863090</td>\n",
       "      <td>8.433458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Season        Date Home_Team Visitor_Team  H_halftime_odds  \\\n",
       "5264        7449    2023  2024-01-21       KAN          BUF         2.386667   \n",
       "5265        7444    2023  2024-01-28       DET          SFO         1.263333   \n",
       "5266        7445    2023  2024-01-28       KAN          BAL         1.345556   \n",
       "5267        7645    2023  2024-02-11       SFO          KAN         1.301111   \n",
       "5268        7808    2024  2024-10-10       SFO          SEA         1.056667   \n",
       "\n",
       "      V_halftime_odds  H_start_odds  V_start_odds  H_Won  D_First_Downs  \\\n",
       "5264         1.577778      2.073398      1.724771    1.0         -5.049   \n",
       "5265         3.894444      4.307745      1.203585    0.0         -0.376   \n",
       "5266         3.222222      2.885004      1.396731    1.0         -1.149   \n",
       "5267         3.472222      1.757009      2.028809    0.0          1.930   \n",
       "5268         9.383333      1.461897      2.642566    1.0         -6.140   \n",
       "\n",
       "      D_Rush  D_Yds   D_TDs  D_Cmp    D_Att   D_Yd    D_TD  D_INT  D_Sacked  \\\n",
       "5264  14.342 -4.790 -10.070 -1.931  -67.962  5.714  52.173 -0.224     3.239   \n",
       "5265  -5.287  1.664 -19.688  6.013  -10.797  3.569  26.740 -0.405     4.852   \n",
       "5266 -29.932 -1.342   3.770  2.065    5.677  3.682   1.174  0.019     0.237   \n",
       "5267  23.153  3.614 -38.602  1.845  -27.846  2.388  45.530  0.263    -3.815   \n",
       "5268  14.184  7.482 -32.831 -7.632 -101.676 -0.471  56.893 -1.129    -1.901   \n",
       "\n",
       "      D_Sacked_Yards  D_Net_Pass_Yards  D_Total_Yards  D_Fumbles  D_Lost  \\\n",
       "5264           1.041           -18.251        -53.359     -0.615   1.029   \n",
       "5265          -0.741            15.220        -15.201      0.130  -0.277   \n",
       "5266          -6.887            15.044        -17.795     -0.440   0.002   \n",
       "5267           6.219            22.362         34.943     -0.535  -0.615   \n",
       "5268          -3.625           -37.717         -7.982      1.507   0.691   \n",
       "\n",
       "      D_Turnovers  D_Penalties  D_Third_Down_Conv  D_Fourth_Down_Conv  \\\n",
       "5264       -0.423       -3.945            -13.555              27.766   \n",
       "5265       -0.445       16.338            -14.753              38.606   \n",
       "5266        0.246        1.906             -1.403              -7.241   \n",
       "5267       -0.271       -6.133             18.912             -20.599   \n",
       "5268       -0.143      -21.287              6.148               1.838   \n",
       "\n",
       "      D_Time_of_Possession  D_passing_att  D_passing_cmp  D_passing_int  \\\n",
       "5264                -4.679          3.271          1.649         -0.306   \n",
       "5265                -1.158          2.920          3.124         -0.397   \n",
       "5266                 0.512          7.909          4.222          0.376   \n",
       "5267                 2.204         -1.002         -0.423          0.194   \n",
       "5268                 0.993        -15.357        -12.401         -0.485   \n",
       "\n",
       "      D_passing_lng  D_passing_sk  D_passing_td  D_receiving_lng  \\\n",
       "5264          0.766         0.376        -0.178           13.745   \n",
       "5265         -4.086        -0.315        -0.269            4.240   \n",
       "5266         -2.455        -0.990        -1.230          -16.767   \n",
       "5267          6.831         0.779         0.370            3.113   \n",
       "5268         -3.408        -0.875         0.452           -3.691   \n",
       "\n",
       "      D_rushing_att  D_rushing_lng  D_rushing_td  D_rushing_yds  \\\n",
       "5264        -10.854         -4.265        -0.947        -35.108   \n",
       "5265         -1.072        -18.576         0.293        -30.421   \n",
       "5266         -3.475        -19.850        -0.497        -32.839   \n",
       "5267          1.567          2.206         0.951         12.581   \n",
       "5268         10.496         -0.232        -1.471         29.735   \n",
       "\n",
       "      D_passing_rushing_td  D_def_interceptions_int  D_def_interceptions_td  \\\n",
       "5264                -1.125                   -1.165                  -0.193   \n",
       "5265                 0.024                   -0.155                  -0.104   \n",
       "5266                -1.727                   -0.833                  -0.034   \n",
       "5267                 1.320                    1.064                   0.080   \n",
       "5268                -1.018                    0.526                   0.460   \n",
       "\n",
       "      D_def_interceptions_yds  D_fumbles_ff  D_fumbles_fr  D_fumbles_td  \\\n",
       "5264                   -8.337         0.421        -0.257         0.277   \n",
       "5265                    0.189         0.511        -0.168         0.000   \n",
       "5266                  -13.352        -0.654        -0.109         0.207   \n",
       "5267                   13.771        -0.396        -0.668        -0.155   \n",
       "5268                   24.419         1.353         2.059         0.000   \n",
       "\n",
       "      D_fumbles_yds   D_sk  D_tackles_ast  D_tackles_comb  D_tackles_solo  \\\n",
       "5264         25.928  0.859         -2.725           5.923           8.648   \n",
       "5265          1.961  0.267         -2.540          -1.567           0.973   \n",
       "5266         14.747  0.109         -8.664          -8.438           0.226   \n",
       "5267        -14.084 -0.516         -2.174          -5.335          -3.160   \n",
       "5268         -0.919  0.618         -1.915          -3.603          -1.688   \n",
       "\n",
       "      D_punting_pnt  D_punting_avg  D_scoring_fga  D_scoring_fgp  \\\n",
       "5264          0.551          2.611          1.207          0.139   \n",
       "5265          0.893          0.897         -0.082          0.198   \n",
       "5266         -0.408          1.400          0.973          0.093   \n",
       "5267          0.132         -1.046         -1.315         -0.326   \n",
       "5268         -1.445         -0.773          0.985          0.460   \n",
       "\n",
       "      D_scoring_xpa  D_scoring_xpp  D_Final  D_Final_Allowed  D_start_odds  \\\n",
       "5264         -1.272         -0.004   -5.790           -1.099         0.349   \n",
       "5265         -0.369         -0.064   -0.484            2.681         3.104   \n",
       "5266         -1.516         -0.004   -8.754           -2.969         1.488   \n",
       "5267          1.278         -0.052    3.839            3.252        -0.272   \n",
       "5268          0.000          0.000    0.062           -6.312        -1.181   \n",
       "\n",
       "      D_halftime_odds  D_datediff  D_pythagorean  kick_punt_umap_dim_1  \\\n",
       "5264            0.809         2.0         -0.073             -0.944908   \n",
       "5265           -2.631        -1.0         -0.178              4.765425   \n",
       "5266           -1.877        -1.0         -0.126              0.746289   \n",
       "5267           -2.171         0.0          0.104              6.982881   \n",
       "5268           -8.327         0.0          0.026             -2.863090   \n",
       "\n",
       "      kick_punt_umap_dim_2  \n",
       "5264              9.706598  \n",
       "5265              5.378789  \n",
       "5266             10.155971  \n",
       "5267              4.331357  \n",
       "5268              8.433458  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_performance_df[:5]\n",
    "# print(test_performance_df['H_start_odds'][:5])\n",
    "# print(test_performance_df['V_start_odds'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfc250",
   "metadata": {},
   "source": [
    "# Columns to use\n",
    "(TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db79ec",
   "metadata": {},
   "source": [
    "## 1. Separate continuous, categorical, and label column names\n",
    "\n",
    "Pretty much everything is continuous. \n",
    "\n",
    "Note: the y_col is what you're trying to predict\n",
    "\n",
    "## Feature engineering\n",
    "New Columns\n",
    "- **h_win**: Home team won\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a3960b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5264, 67)\n",
      "(5264, 48)\n",
      "(191, 5)\n",
      "      H_Won  H_start_odds  V_start_odds  H_halftime_odds  V_halftime_odds\n",
      "5459    0.0      1.724771      2.073398              0.0              0.0\n",
      "5460    0.0      4.991514      1.159063              0.0              0.0\n",
      "5461    1.0      4.513858      1.188369              0.0              0.0\n",
      "5462    0.0      3.804017      1.250000              0.0              0.0\n",
      "5463    0.0      1.790476      1.986097              0.0              0.0\n"
     ]
    }
   ],
   "source": [
    "cat_cols = []\n",
    "\n",
    "cont_cols = [col for col in nfl_utils.cont_cols if col not in nfl_utils.drop_cols]\n",
    "\n",
    "\n",
    "y_col = ['H_Won'] # Old\n",
    "y_col = ['H_Won', 'H_start_odds', 'V_start_odds']\n",
    "y_col_perf = ['H_Won', 'H_start_odds', 'V_start_odds', 'H_halftime_odds', 'V_halftime_odds']\n",
    "\n",
    "\n",
    "# create cont_df and y_df from the df\n",
    "print(df.shape)\n",
    "cont_df = df[cont_cols]\n",
    "y_df = df[y_col]\n",
    "\n",
    "# test performance set\n",
    "perf_conts_df = test_performance_df[cont_cols]\n",
    "perf_y_df = test_performance_df[y_col_perf]\n",
    "perf_date_df = test_performance_df[['Date','Home_Team', 'Visitor_Team']]\n",
    "\n",
    "# print(cont_df.dtypes)\n",
    "print(cont_df.shape)\n",
    "print(perf_y_df.shape)\n",
    "print(perf_y_df.tail())\n",
    "# print(perf_conts_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a936f59-3205-422f-b760-7296518b2739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      H_Won  H_start_odds  V_start_odds  H_halftime_odds  V_halftime_odds\n",
      "5264    1.0      2.073398      1.724771         2.386667         1.577778\n",
      "5265    0.0      4.307745      1.203585         1.263333         3.894444\n",
      "5266    1.0      2.885004      1.396731         1.345556         3.222222\n",
      "5267    0.0      1.757009      2.028809         1.301111         3.472222\n",
      "5268    1.0      1.461897      2.642566         1.056667         9.383333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D_First_Downs</th>\n",
       "      <th>D_Rush</th>\n",
       "      <th>D_Yds</th>\n",
       "      <th>D_TDs</th>\n",
       "      <th>D_Cmp</th>\n",
       "      <th>D_Att</th>\n",
       "      <th>D_Yd</th>\n",
       "      <th>D_INT</th>\n",
       "      <th>D_Sacked</th>\n",
       "      <th>D_Sacked_Yards</th>\n",
       "      <th>D_Total_Yards</th>\n",
       "      <th>D_Fumbles</th>\n",
       "      <th>D_Lost</th>\n",
       "      <th>D_Turnovers</th>\n",
       "      <th>D_Penalties</th>\n",
       "      <th>D_Fourth_Down_Conv</th>\n",
       "      <th>D_Time_of_Possession</th>\n",
       "      <th>D_passing_att</th>\n",
       "      <th>D_passing_cmp</th>\n",
       "      <th>D_passing_int</th>\n",
       "      <th>D_passing_lng</th>\n",
       "      <th>D_passing_sk</th>\n",
       "      <th>D_passing_td</th>\n",
       "      <th>D_receiving_lng</th>\n",
       "      <th>D_rushing_att</th>\n",
       "      <th>D_rushing_lng</th>\n",
       "      <th>D_rushing_td</th>\n",
       "      <th>D_rushing_yds</th>\n",
       "      <th>D_def_interceptions_int</th>\n",
       "      <th>D_def_interceptions_td</th>\n",
       "      <th>D_def_interceptions_yds</th>\n",
       "      <th>D_fumbles_fr</th>\n",
       "      <th>D_fumbles_td</th>\n",
       "      <th>D_fumbles_yds</th>\n",
       "      <th>D_sk</th>\n",
       "      <th>D_tackles_ast</th>\n",
       "      <th>D_tackles_comb</th>\n",
       "      <th>D_tackles_solo</th>\n",
       "      <th>kick_punt_umap_dim_1</th>\n",
       "      <th>kick_punt_umap_dim_2</th>\n",
       "      <th>D_punting_pnt</th>\n",
       "      <th>D_punting_avg</th>\n",
       "      <th>D_scoring_fga</th>\n",
       "      <th>D_scoring_fgp</th>\n",
       "      <th>D_scoring_xpa</th>\n",
       "      <th>D_scoring_xpp</th>\n",
       "      <th>D_pythagorean</th>\n",
       "      <th>D_start_odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.265</td>\n",
       "      <td>5.140</td>\n",
       "      <td>-8.261</td>\n",
       "      <td>31.290</td>\n",
       "      <td>-8.121</td>\n",
       "      <td>43.430</td>\n",
       "      <td>4.074</td>\n",
       "      <td>-0.684</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-4.750</td>\n",
       "      <td>134.107</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>22.515</td>\n",
       "      <td>35.846</td>\n",
       "      <td>-1.945</td>\n",
       "      <td>15.846</td>\n",
       "      <td>8.055</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>34.941</td>\n",
       "      <td>-2.048</td>\n",
       "      <td>1.702</td>\n",
       "      <td>32.967</td>\n",
       "      <td>-6.551</td>\n",
       "      <td>24.316</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-11.221</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.276</td>\n",
       "      <td>10.643</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-4.599</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.291422</td>\n",
       "      <td>9.552850</td>\n",
       "      <td>0.599</td>\n",
       "      <td>-1.273</td>\n",
       "      <td>0.662</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>2.044</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.114</td>\n",
       "      <td>1.181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.629</td>\n",
       "      <td>-29.721</td>\n",
       "      <td>-20.239</td>\n",
       "      <td>105.471</td>\n",
       "      <td>-4.695</td>\n",
       "      <td>135.596</td>\n",
       "      <td>-6.074</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-6.794</td>\n",
       "      <td>-1.007</td>\n",
       "      <td>-58.301</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.941</td>\n",
       "      <td>-2.555</td>\n",
       "      <td>13.787</td>\n",
       "      <td>-5.022</td>\n",
       "      <td>-2.121</td>\n",
       "      <td>1.048</td>\n",
       "      <td>1.272</td>\n",
       "      <td>19.176</td>\n",
       "      <td>1.702</td>\n",
       "      <td>0.835</td>\n",
       "      <td>7.434</td>\n",
       "      <td>-14.743</td>\n",
       "      <td>-19.537</td>\n",
       "      <td>-2.132</td>\n",
       "      <td>-82.452</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.096</td>\n",
       "      <td>1.397</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-11.360</td>\n",
       "      <td>-1.272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.618100</td>\n",
       "      <td>7.267665</td>\n",
       "      <td>-0.864</td>\n",
       "      <td>5.719</td>\n",
       "      <td>1.636</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-1.904</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>1.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.647</td>\n",
       "      <td>-7.419</td>\n",
       "      <td>-4.353</td>\n",
       "      <td>41.000</td>\n",
       "      <td>3.283</td>\n",
       "      <td>62.070</td>\n",
       "      <td>-4.474</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-3.724</td>\n",
       "      <td>-4.224</td>\n",
       "      <td>65.592</td>\n",
       "      <td>-1.029</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-1.324</td>\n",
       "      <td>-13.379</td>\n",
       "      <td>13.787</td>\n",
       "      <td>-1.588</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>2.772</td>\n",
       "      <td>-2.246</td>\n",
       "      <td>0.033</td>\n",
       "      <td>21.437</td>\n",
       "      <td>-3.029</td>\n",
       "      <td>10.765</td>\n",
       "      <td>-0.415</td>\n",
       "      <td>-4.886</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-10.676</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>-21.176</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.573103</td>\n",
       "      <td>1.891357</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-4.020</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-1.007</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.827</td>\n",
       "      <td>-3.625</td>\n",
       "      <td>2.011</td>\n",
       "      <td>-21.217</td>\n",
       "      <td>5.103</td>\n",
       "      <td>37.158</td>\n",
       "      <td>7.551</td>\n",
       "      <td>2.173</td>\n",
       "      <td>-2.059</td>\n",
       "      <td>-13.463</td>\n",
       "      <td>144.618</td>\n",
       "      <td>0.544</td>\n",
       "      <td>1.187</td>\n",
       "      <td>3.801</td>\n",
       "      <td>-11.202</td>\n",
       "      <td>49.173</td>\n",
       "      <td>5.379</td>\n",
       "      <td>14.000</td>\n",
       "      <td>10.915</td>\n",
       "      <td>3.441</td>\n",
       "      <td>21.096</td>\n",
       "      <td>-3.386</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>47.860</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-5.044</td>\n",
       "      <td>1.118</td>\n",
       "      <td>-11.184</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.165</td>\n",
       "      <td>8.996</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.044179</td>\n",
       "      <td>8.739308</td>\n",
       "      <td>-4.794</td>\n",
       "      <td>-4.150</td>\n",
       "      <td>2.202</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-5.646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.110</td>\n",
       "      <td>-15.445</td>\n",
       "      <td>10.092</td>\n",
       "      <td>-12.125</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-82.890</td>\n",
       "      <td>1.221</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-3.026</td>\n",
       "      <td>0.585</td>\n",
       "      <td>13.544</td>\n",
       "      <td>0.518</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>11.952</td>\n",
       "      <td>6.434</td>\n",
       "      <td>1.794</td>\n",
       "      <td>-8.794</td>\n",
       "      <td>-8.033</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-3.371</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-1.228</td>\n",
       "      <td>-34.746</td>\n",
       "      <td>9.721</td>\n",
       "      <td>-6.382</td>\n",
       "      <td>0.754</td>\n",
       "      <td>40.412</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-18.860</td>\n",
       "      <td>-1.121</td>\n",
       "      <td>0.276</td>\n",
       "      <td>1.985</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.613956</td>\n",
       "      <td>2.804782</td>\n",
       "      <td>-1.349</td>\n",
       "      <td>-5.401</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1.181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   D_First_Downs  D_Rush   D_Yds    D_TDs  D_Cmp    D_Att   D_Yd  D_INT  \\\n",
       "0          2.265   5.140  -8.261   31.290 -8.121   43.430  4.074 -0.684   \n",
       "1         -3.629 -29.721 -20.239  105.471 -4.695  135.596 -6.074 -0.011   \n",
       "2          2.647  -7.419  -4.353   41.000  3.283   62.070 -4.474 -0.184   \n",
       "3          6.827  -3.625   2.011  -21.217  5.103   37.158  7.551  2.173   \n",
       "4          3.110 -15.445  10.092  -12.125 -0.239  -82.890  1.221  0.368   \n",
       "\n",
       "   D_Sacked  D_Sacked_Yards  D_Total_Yards  D_Fumbles  D_Lost  D_Turnovers  \\\n",
       "0    -0.044          -4.750        134.107     -0.342  -0.103       -1.526   \n",
       "1    -6.794          -1.007        -58.301      0.239   0.537        0.941   \n",
       "2    -3.724          -4.224         65.592     -1.029  -0.110       -1.324   \n",
       "3    -2.059         -13.463        144.618      0.544   1.187        3.801   \n",
       "4    -3.026           0.585         13.544      0.518  -0.544       -0.162   \n",
       "\n",
       "   D_Penalties  D_Fourth_Down_Conv  D_Time_of_Possession  D_passing_att  \\\n",
       "0       22.515              35.846                -1.945         15.846   \n",
       "1       -2.555              13.787                -5.022         -2.121   \n",
       "2      -13.379              13.787                -1.588          0.430   \n",
       "3      -11.202              49.173                 5.379         14.000   \n",
       "4       11.952               6.434                 1.794         -8.794   \n",
       "\n",
       "   D_passing_cmp  D_passing_int  D_passing_lng  D_passing_sk  D_passing_td  \\\n",
       "0          8.055         -1.257         34.941        -2.048         1.702   \n",
       "1          1.048          1.272         19.176         1.702         0.835   \n",
       "2          0.305         -0.460          2.772        -2.246         0.033   \n",
       "3         10.915          3.441         21.096        -3.386        -0.460   \n",
       "4         -8.033         -0.360         -3.371         0.007        -1.228   \n",
       "\n",
       "   D_receiving_lng  D_rushing_att  D_rushing_lng  D_rushing_td  D_rushing_yds  \\\n",
       "0           32.967         -6.551         24.316        -0.099        -11.221   \n",
       "1            7.434        -14.743        -19.537        -2.132        -82.452   \n",
       "2           21.437         -3.029         10.765        -0.415         -4.886   \n",
       "3           47.860         -0.140         -5.044         1.118        -11.184   \n",
       "4          -34.746          9.721         -6.382         0.754         40.412   \n",
       "\n",
       "   D_def_interceptions_int  D_def_interceptions_td  D_def_interceptions_yds  \\\n",
       "0                    0.180                   0.276                   10.643   \n",
       "1                   -0.820                   0.000                    2.096   \n",
       "2                   -0.952                  -0.165                  -10.676   \n",
       "3                    0.614                   0.165                    8.996   \n",
       "4                   -0.044                  -0.276                  -18.860   \n",
       "\n",
       "   D_fumbles_fr  D_fumbles_td  D_fumbles_yds   D_sk  D_tackles_ast  \\\n",
       "0        -0.835         0.000         -4.599  0.526            0.0   \n",
       "1         1.397        -0.165        -11.360 -1.272            0.0   \n",
       "2        -0.140        -0.460        -21.176 -0.165            0.0   \n",
       "3         0.937         0.000          0.662  0.732            0.0   \n",
       "4        -1.121         0.276          1.985 -0.298            0.0   \n",
       "\n",
       "   D_tackles_comb  D_tackles_solo  kick_punt_umap_dim_1  kick_punt_umap_dim_2  \\\n",
       "0             0.0             0.0              5.291422              9.552850   \n",
       "1             0.0             0.0              7.618100              7.267665   \n",
       "2             0.0             0.0              2.573103              1.891357   \n",
       "3             0.0             0.0              4.044179              8.739308   \n",
       "4             0.0             0.0              3.613956              2.804782   \n",
       "\n",
       "   D_punting_pnt  D_punting_avg  D_scoring_fga  D_scoring_fgp  D_scoring_xpa  \\\n",
       "0          0.599         -1.273          0.662         -0.094          2.044   \n",
       "1         -0.864          5.719          1.636         -0.153         -1.904   \n",
       "2         -0.989         -4.020          0.901          0.242         -1.007   \n",
       "3         -4.794         -4.150          2.202          0.255         -0.390   \n",
       "4         -1.349         -5.401         -0.890          0.103         -0.022   \n",
       "\n",
       "   D_scoring_xpp  D_pythagorean  D_start_odds  \n",
       "0          0.441          0.114         1.181  \n",
       "1          0.099         -0.063         1.330  \n",
       "2         -0.526          0.179         0.741  \n",
       "3         -0.360          0.248        -5.646  \n",
       "4          0.000          0.007         1.181  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(perf_y_df[:5])\n",
    "cont_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05613417",
   "metadata": {},
   "source": [
    "#### 1a. Normalize cont_df\n",
    "StandardScaler is instead used by the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7e76a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# cont_scaled = min_max_scaler.fit_transform(cont_df.values)\n",
    "# cont_df = pd.DataFrame(cont_scaled)\n",
    "# cont_df.head()\n",
    "\n",
    "# # test performance set\n",
    "# perf_conts_df_scaled = min_max_scaler.fit_transform(perf_conts_df.values)\n",
    "# perf_conts_df = pd.DataFrame(perf_conts_df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1912ae",
   "metadata": {},
   "source": [
    "### 3. Create an array of continuous values\n",
    "Numpy array 'conts' containing stack of each continuous column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c326ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "conts = np.stack([cont_df[col].values for col in list(cont_df.columns)], 1)\n",
    "conts[:5]\n",
    "\n",
    "y_col = np.stack([y_df[col].values for col in y_col], 1)\n",
    "\n",
    "# test performance set\n",
    "perf_conts = np.stack([perf_conts_df[col].values for col in list(perf_conts_df.columns)], 1)\n",
    "perf_y_col = np.stack([perf_y_df[col].values for col in list(perf_y_df.columns)], 1)\n",
    "perf_date_col = np.stack([perf_date_df[col].values for col in list(perf_date_df.columns)], 1)\n",
    "\n",
    "\n",
    "conts_train = conts\n",
    "y_train = y_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2547a",
   "metadata": {},
   "source": [
    "### 4. Convert conts to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b7db899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5264, 48)\n",
      "(5264, 3)\n"
     ]
    }
   ],
   "source": [
    "print(conts.shape)\n",
    "print(y_col.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ac789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handled by model.fit()\n",
    "# conts = torch.tensor(conts, dtype=torch.float32)\n",
    "# y_col = torch.tensor(y_col, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb278a38-cb66-41d3-9652-85996190807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Want: \n",
    "    An array > 1 elements for PCC formula\n",
    "    A smaller array for kelly_criterion\n",
    "    \n",
    "\n",
    "\n",
    "x: 1d array of predictions between -1 and 1 where negative number means visitor predicted to win\n",
    "y: ['H_Won', 'H_start_odds', 'V_start_odds']\n",
    "pearson_multiplier: constant to multiply the pearson correlation coefficient's result by\n",
    "max_bet_size: Amount to multiply to kelly criterion\n",
    "\"\"\"\n",
    "def nfl_custom_criterion(x, y, pearson_multiplier=0.5, max_bet_size=100):\n",
    "    # ------------------------------------------------\n",
    "    # Preliminary calculations\n",
    "    # ------------------------------------------------\n",
    "    h_start_odds = y[:,1]\n",
    "    v_start_odds = y[:,2]\n",
    "    h_won = y[:,0]\n",
    "    y_decimal_odds = torch.where(x > 0, h_start_odds, v_start_odds) # Predicted vs actual odds (regardless of correct prediction)\n",
    "    y_prob = 1 / y_decimal_odds                  # Probability (regardless of correct prediction)\n",
    "    x_H_Won = torch.round(torch.sigmoid(20 * x)) # Sigmoid so that it's differentiable. The 20 is arbitrarily large number\n",
    "    y_correct_prediction = torch.abs((x_H_Won - h_won))        # 1 if wrong bet, otherwise 0. Used to reset kelly when wrong\n",
    "    y_correct_prediction_mult_two = 2 * y_correct_prediction   # 2 if wrong bet, 0 if correct\n",
    "    x = torch.abs(x)\n",
    "\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # 1. Calculate the Pearson Correlation Coefficient\n",
    "    # ------------------------------------------------\n",
    "    n = x.size(0)\n",
    "    sum_x = torch.sum(x)\n",
    "    sum_x_squared = torch.sum(x**2)\n",
    "    sum_y = torch.sum(y_prob)\n",
    "    sum_y_squared = torch.sum(y_prob**2)\n",
    "    sum_pow_x = torch.sum(x**2)\n",
    "    sum_pow_y = torch.sum(y_prob**2)\n",
    "    x_mul_y = torch.mul(x, y_prob)\n",
    "    sum_x_mul_y = torch.sum(x_mul_y)\n",
    "\n",
    "    \n",
    "    # PCC Formula (eps to avoid NaN)\n",
    "    eps = 1e-8\n",
    "    pcc_numerator = n * sum_x_mul_y - sum_x * sum_y\n",
    "    pcc_denominator_one = torch.sqrt(n * sum_pow_x - sum_x_squared + eps)\n",
    "    pcc_denominator_two = torch.sqrt(n * sum_pow_y - sum_y_squared + eps)\n",
    "    pcc = pcc_numerator / (pcc_denominator_one * pcc_denominator_two + eps)\n",
    "    pcc = pearson_multiplier * torch.abs(pcc)\n",
    "\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # 2. Calculate the kelly criterion\n",
    "    #    Entirely wrong predictions are negated and kept in \"incorrect_bets\" (pcc not applied to wrong predictions)\n",
    "    #    Correct predictions are kept in \"correct_bets\". Pcc is applied to this & stored in pcc_adjusted_correct_bets\n",
    "    #    Possible issue: This always bets max_bet_size\n",
    "    #    The result is cumulatively calculated. i.e. The sum of the previous values are used to calculate the next one\n",
    "    # ------------------------------------------------\n",
    "    kelly_criterion = x - ((1 - x) / y_decimal_odds)\n",
    "    bet_multiplier = torch.clamp(kelly_criterion, min=0)   # Kelly results that are negative are ignored\n",
    "    # bet_unadjusted_profit = bet_multiplier*max_bet_size    # Assumes all bets were correct\n",
    "\n",
    "    #correct_bets = bet_unadjusted_profit - (bet_unadjusted_profit * y_correct_prediction)   # All correct bets after kelly, profit or 0.\n",
    "    #pcc_adjusted_correct_bets = correct_bets * (1 - pcc)                                    # \"correct_bets\" penalized by pcc\n",
    "    #incorrect_bets = bet_unadjusted_profit - (bet_unadjusted_profit * y_correct_prediction_mult_two) # Negative numbers are incorrect bets\n",
    "    #incorrect_bets = torch.clamp(incorrect_bets, max=0)  # Restrict to 0 or negative.\n",
    "    #combined_bets = correct_bets + incorrect_bets        # Profit\n",
    "\n",
    "\n",
    "    # 4/5/25 adjustment of kelly\n",
    "    #    Want to use cumprod. Cumsum does nothing and is the same as torch.sum in this scenario?\n",
    "    #    Basically start with max_bet_size and return as if you made the bets sequentially\n",
    "    correct_bet_multiplier = bet_multiplier - (bet_multiplier * y_correct_prediction)            # Correct bets after kelly. Bet multiplier or 0\n",
    "    correct_bet_multiplier = correct_bet_multiplier * (1 - pcc)                                  # \"correct_bet_multiplier\" penalized by pcc\n",
    "    incorrect_bet_multiplier = bet_multiplier - (bet_multiplier * y_correct_prediction_mult_two) # Negative numbers are incorrect bets\n",
    "    incorrect_bet_multiplier = torch.clamp(incorrect_bet_multiplier, max=0)                      # Restrict to 0 or negative\n",
    "    print(torch.max(bet_multiplier))\n",
    "    combined_bet_multiplier = correct_bet_multiplier + incorrect_bet_multiplier                  # Combine correct & incorrect bet multipliers\n",
    "    combined_bet_multiplier = combined_bet_multiplier + 1                                        # Converts to format friendly to cumprod\n",
    "                                                                                                 # Ex: loss=-0.3, profit=0.3 --> loss=0.7, profit=1.3\n",
    "\n",
    "    # Prepend max_bet_size to the tensor before torch.cumprod\n",
    "    start_bet = torch.tensor([max_bet_size], dtype=torch.float32) # Creates a tensor containing max_bet_size\n",
    "    combined_bet_multiplier = torch.cat((start_bet, combined_bet_multiplier))   # Prepends max_bet_size to the combined_bet_multiplier tensor\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # Combine & Return\n",
    "    #     Negate everything for Adam & optuna\n",
    "    # ------------------------------------------------\n",
    "    return torch.cumprod(combined_bet_multiplier, dim=0)[-1]\n",
    "    # return -torch.cumsum(combined_bets, dim=0)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9875b494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TabularModelUpdated(nn.Module, BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_cont, out_sz, layer_shape, p=0.5, # criterion=nn.MSELoss(),\n",
    "                optimizer_class=torch.optim.Adam, lr=0.001, confidence_threshold=0.1,\n",
    "                batch_size=1000):\n",
    "        super().__init__()\n",
    "        # Model architecture params\n",
    "        self.layer_shape = layer_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.n_cont = n_cont\n",
    "        self.out_sz = out_sz\n",
    "        self.p = p\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Training params\n",
    "        # self.criterion = criterion\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        # BatchNorm layer for continuous data\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        # Variable that holds the list of layers\n",
    "        layerlist = []\n",
    "        n_in = n_cont # no embed again\n",
    "        # Iterate through the passed in \"layers\" parameter (ie, [200,100]) to build a list of layers\n",
    "        for i, width in enumerate(self.layer_shape):\n",
    "            # First layer gets special treatment\n",
    "            if i == 0:\n",
    "                layerlist.extend([\n",
    "                    nn.Linear(n_in, width),\n",
    "                    nn.Mish(),  # Mish instead of ReLU\n",
    "                    nn.BatchNorm1d(width),\n",
    "                    nn.Dropout(p/2)  # Less dropout in earlier layers\n",
    "                ])\n",
    "            else:\n",
    "                layerlist.extend([\n",
    "                    nn.Linear(n_in, width),\n",
    "                    nn.Mish(),\n",
    "                    nn.BatchNorm1d(width),\n",
    "                    nn.Dropout(p)\n",
    "                ])\n",
    "            n_in = width\n",
    "        # layerlist.append(nn.Linear(layers[-1], out_sz))\n",
    "        \n",
    "        # Final layer\n",
    "        layerlist.extend([\n",
    "            nn.Linear(self.layer_shape[-1], out_sz),\n",
    "            # nn.Softmax()  # Ensures output between 0 and 1\n",
    "        ])\n",
    "        # Convert the list of layers into an attribute\n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "        # Initialize the optimizer\n",
    "        self.optimizer = optimizer_class(self.parameters(), lr=self.lr)\n",
    "\n",
    "        \n",
    "    def forward(self, x_cont):\n",
    "        x_cont = self.bn_cont(x_cont)  # Normalize the incoming continuous data\n",
    "        x = self.layers(x_cont)        # Set up model layers\n",
    "        return torch.clamp(x, -1, 1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        For sklearn pipeline\n",
    "        \"\"\"\n",
    "        # Convert X,y to torch.tensor if needed\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.FloatTensor(X)\n",
    "        if not isinstance(y, torch.Tensor):\n",
    "            y = torch.FloatTensor(y)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        # optimizer = self.optimizer_class(self.parameters(), lr=self.lr)\n",
    "\n",
    "        dataset = TensorDataset(X, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        # Training loop\n",
    "        self.train()\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            self.optimizer.zero_grad()\n",
    "            y_pred = self.forward(X_batch)[:,0]\n",
    "            y_pred.requires_grad_()\n",
    "            y_batch.requires_grad_()\n",
    "            #loss = self.criterion(y_pred, y_batch)\n",
    "            # loss = test_custom_loss(y_pred, y_batch)\n",
    "            loss = nfl_custom_criterion(y_pred, y_batch)\n",
    "            #print(loss)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return probas\n",
    "        # print(\"predict\")\n",
    "        # return (probas > 0.5).astype(int)\n",
    "        # return probas\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            if not isinstance(X, torch.Tensor):\n",
    "                X = torch.FloatTensor(X)\n",
    "            return self(X)\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        12/5 - this isn't called at all if 'scoring' is defined\n",
    "        \"\"\"\n",
    "        probas = self.predict_proba(X)[:, 0]\n",
    "        result = nfl_custom_criterion(probas, y)\n",
    "        return result.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94b31ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # --- Suggest hyperparameters ---\n",
    "\n",
    "    # criterion = trial.suggest_categorical('criterion', nfl_utils.map_losses(None).keys())\n",
    "    batch_size = trial.suggest_categorical('batch_size', [100, 400, 1000, 3000])\n",
    "    first_layer_size = trial.suggest_categorical('first_layer_size', [64, 56, 48, 32, 16, 12])\n",
    "    min_layers = math.floor(math.sqrt(first_layer_size))\n",
    "    num_layers = trial.suggest_int('num_layers', 2, min_layers)\n",
    "    confidence_threshold = trial.suggest_float('confidence_threshold', 0, 0.05)\n",
    "    layer_shape = [first_layer_size]\n",
    "    for i in range(1, num_layers):\n",
    "        layer_shape.append(first_layer_size//(2*i))\n",
    "    \n",
    "    # Set random state to have consistent results (42 is arbitrary)\n",
    "    set_all_seeds()\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "    # Split once\n",
    "    X_train_fold = []\n",
    "    X_val = []\n",
    "    y_train_fold = []\n",
    "    y_val = []\n",
    "    models = []\n",
    "    for train_index, val_index in kf.split(conts_train):\n",
    "        # print(f\"train {train_index.shape} val {val_index.shape}\")\n",
    "        X_train_fold.append(torch.FloatTensor(conts_train[train_index]).to(device))\n",
    "        X_val.append(torch.FloatTensor(conts_train[val_index]).to(device))\n",
    "\n",
    "        y_train_fold.append(torch.FloatTensor(y_train[train_index]).to(device))\n",
    "        y_val.append(torch.FloatTensor(y_train[val_index]).to(device))\n",
    "\n",
    "        model = TabularModelUpdated(\n",
    "            n_cont=conts.shape[1],\n",
    "            out_sz=1,\n",
    "            layer_shape=layer_shape,\n",
    "            p=trial.suggest_float('dropout', 0.28, 0.38),     # Dropout\n",
    "            # criterion=nfl_utils.map_losses(criterion),\n",
    "            optimizer_class=torch.optim.Adam,\n",
    "            lr=trial.suggest_float('lr', 1e-3, 1e-2, log=True),   # Learning rate \n",
    "            confidence_threshold=confidence_threshold,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()), # Standardize the numerical features\n",
    "            # ('regressor', LinearRegression()), # Apply a regression model\n",
    "            ('model', model)\n",
    "        ])\n",
    "        models.append(pipeline)\n",
    "\n",
    "    # Run once on each split, track average loss, stop if > max patience\n",
    "    max_patience = 10\n",
    "    current_patience = max_patience\n",
    "    tracked_loss = 0.0\n",
    "    n_epochs = 0\n",
    "    while current_patience > 0 or n_epochs < 50:\n",
    "        n_epochs = n_epochs + 1\n",
    "        running_loss = []\n",
    "        for i in range(0,n_splits):\n",
    "            # ----- Train -----\n",
    "            models[i].fit(X_train_fold[i], y_train_fold[i])\n",
    "\n",
    "            # ----- Eval -----\n",
    "            # print(f\"los: {models[i].score(X_val[i], y_val[i])} type: {type(models[i].score(X_val[i], y_val[i]))}\")\n",
    "            running_loss.append(models[i].score(X_val[i], y_val[i]))\n",
    "            # y_pred = models[i].predict(X_val[i])\n",
    "            # running_loss.append(f1_score(y_val[i], y_pred))\n",
    "        # print(f\"{running_loss} at {n_epochs}\")\n",
    "        running_loss = np.mean(running_loss)\n",
    "        # print(f\"rloss: {running_loss}\")\n",
    "        \n",
    "        # ----- Current epoch loss < previous -----\n",
    "        # print(f\"{tracked_loss} {running_loss} {tracked_loss > running_loss}\")\n",
    "        if tracked_loss > running_loss:\n",
    "            current_patience = max_patience\n",
    "            tracked_loss = running_loss\n",
    "        else:\n",
    "            current_patience = current_patience - 1\n",
    "    trial.suggest_int('n_epochs', n_epochs, n_epochs)\n",
    "    trial.report(tracked_loss, n_epochs)\n",
    "    return tracked_loss\n",
    "\n",
    "def print_callback(study, trial):\n",
    "    print(f\"Trial {trial.number} finished with value: {trial.value}\")\n",
    "    print(f\"Best trial so far: {study.best_trial.number}, value: {study.best_trial.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd663af5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-05 14:56:06,094] A new study created in memory with name: no-name-17602b76-59dc-4d8c-8a3b-ec2c6b135eba\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(0.7146)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(0.9930)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(0.2814)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(0.6703)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-05 14:56:10,428] Trial 0 failed with parameters: {'batch_size': 400, 'first_layer_size': 16, 'num_layers': 2, 'confidence_threshold': 0.015887862317049257, 'dropout': 0.28089261556214634, 'lr': 0.0015395213596350475} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/forbesjon2/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/k0/7b4qgkdx2vb9ml4ktdckf0hc0000gn/T/ipykernel_11920/1368567117.py\", line 61, in objective\n",
      "    models[i].fit(X_train_fold[i], y_train_fold[i])\n",
      "  File \"/Users/forbesjon2/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/var/folders/k0/7b4qgkdx2vb9ml4ktdckf0hc0000gn/T/ipykernel_11920/1960948281.py\", line 80, in fit\n",
      "    y_pred = self.forward(X_batch)[:,0]\n",
      "  File \"/var/folders/k0/7b4qgkdx2vb9ml4ktdckf0hc0000gn/T/ipykernel_11920/1960948281.py\", line 57, in forward\n",
      "    x = self.layers(x_cont)        # Set up model layers\n",
      "  File \"/Users/forbesjon2/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/forbesjon2/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/forbesjon2/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/modules/container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "  File \"/Users/forbesjon2/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/forbesjon2/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/forbesjon2/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/modules/dropout.py\", line 59, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/Users/forbesjon2/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/functional.py\", line 1295, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-05 14:56:10,431] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Uncomment to run\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# study.optimize(objective, n_trials=3)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[62], line 61\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     58\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,n_splits):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# ----- Train -----\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# ----- Eval -----\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# print(f\"los: {models[i].score(X_val[i], y_val[i])} type: {type(models[i].score(X_val[i], y_val[i]))}\")\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     running_loss\u001b[38;5;241m.\u001b[39mappend(models[i]\u001b[38;5;241m.\u001b[39mscore(X_val[i], y_val[i]))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/sklearn/pipeline.py:394\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    393\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[61], line 80\u001b[0m, in \u001b[0;36mTabularModelUpdated.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 80\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     81\u001b[0m     y_pred\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[1;32m     82\u001b[0m     y_batch\u001b[38;5;241m.\u001b[39mrequires_grad_()\n",
      "Cell \u001b[0;32mIn[61], line 57\u001b[0m, in \u001b[0;36mTabularModelUpdated.forward\u001b[0;34m(self, x_cont)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_cont):\n\u001b[1;32m     56\u001b[0m     x_cont \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn_cont(x_cont)  \u001b[38;5;66;03m# Normalize the incoming continuous data\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cont\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# Set up model layers\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mclamp(x, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.9/site-packages/torch/nn/functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    pruner=optuna.pruners.HyperbandPruner(\n",
    "        min_resource=1,\n",
    "        max_resource=1000\n",
    "    )\n",
    ")\n",
    "# Uncomment to run\n",
    "if True:\n",
    "    study.optimize(objective, n_trials=200, callbacks=[print_callback])\n",
    "    # study.optimize(objective, n_trials=3)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"Value: \", trial.value)\n",
    "    print(\"Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ea5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = study.best_trial.params\n",
    "best_params = {'criterion': 'MSELoss', 'first_layer_size': 64, 'num_layers': 7, 'confidence_threshold': 1.8653941637231173e-05, 'dropout': 0.3181431308672629, 'lr': 0.009969598746996452, 'n_epochs': 231}\n",
    "best_params = {'criterion': 'L1Loss', 'first_layer_size': 16, 'num_layers': 4, 'confidence_threshold': 0.010392255577263067, 'dropout': 0.3018270504618998, 'lr': 0.0020927261150450226, 'n_epochs': 100}\n",
    "best_params = {'batch_size': 400, 'first_layer_size': 12, 'num_layers': 3, 'confidence_threshold': 0.0074379232974536025, 'dropout': 0.3012002579065517, 'lr': 0.002814549768232188, 'n_epochs': 50}\n",
    "best_params = {'batch_size': 100, 'first_layer_size': 12, 'num_layers': 3, 'confidence_threshold': 0.005678565715938724, 'dropout': 0.3541269645351805, 'lr': 0.008447985997640347, 'n_epochs': 53}\n",
    "layer_shape = [best_params['first_layer_size']]\n",
    "for i in range(1, best_params['num_layers']):\n",
    "    layer_shape.append(best_params['first_layer_size']//(2*i))\n",
    "\n",
    "# Set random state to have consistent results (42 is arbitrary)\n",
    "set_all_seeds()\n",
    "model = TabularModelUpdated(\n",
    "    n_cont=conts.shape[1],\n",
    "    out_sz=1,\n",
    "    layer_shape=layer_shape,\n",
    "    p=best_params['dropout'],     # Dropout\n",
    "    # criterion=nfl_utils.map_losses(best_params['criterion']),\n",
    "    optimizer_class=torch.optim.Adam,\n",
    "    lr= best_params['lr'],   # Learning rate \n",
    "    confidence_threshold=best_params['confidence_threshold'],\n",
    "    batch_size=best_params['batch_size']\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Standardize the numerical features\n",
    "    # ('regressor', LinearRegression()), # Apply a regression model\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "perf_y_col_tensor = torch.FloatTensor(perf_y_col).to(device)\n",
    "\n",
    "# Run once on each split, track average loss, stop if > max patience\n",
    "for _ in range(0, (best_params['n_epochs'] - 10)):\n",
    "    running_loss = 0.0\n",
    "    # ----- Train -----\n",
    "    pipeline.fit(conts_train, y_train)\n",
    "\n",
    "    # ----- Eval -----\n",
    "    loss = pipeline.score(perf_conts, perf_y_col_tensor)\n",
    "    print(f\"loss: {loss}\")\n",
    "\n",
    "# pipeline.fit(conts_train, y_train)\n",
    "probas = pipeline.predict(perf_conts)\n",
    "confidence_threshold = best_params['confidence_threshold']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92020224-b9cf-4c61-9dbc-3d0a536d2286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2024-10-14' 'BUF' 'NYJ']\n",
      "2024-10-14: w_odds:1.36 acct_val: 1000.00 usable cash: 900.00 won: False\n",
      "2024-10-14: w_odds:1.43 acct_val: 900.00 usable cash: 810.00 won: True\n",
      "2024-10-14: w_odds:1.58 acct_val: 938.57 usable cash: 716.14 won: True\n",
      "2024-10-14: w_odds:1.46 acct_val: 993.24 usable cash: 616.82 won: False\n",
      "2024-10-14: w_odds:1.30 acct_val: 893.92 usable cash: 527.43 won: False\n",
      "2024-10-14: w_odds:1.25 acct_val: 804.53 usable cash: 446.97 won: False\n",
      "2024-10-14: w_odds:1.33 acct_val: 724.07 usable cash: 374.57 won: True\n",
      "['2024-10-17' 'DEN' 'NOR']\n",
      "2024-10-17: w_odds:1.83 acct_val: 747.94 usable cash: 673.14 won: True\n",
      "['2024-10-20' 'PHI' 'NYG']\n",
      "2024-10-20: w_odds:1.58 acct_val: 810.19 usable cash: 729.17 won: False\n",
      "['2024-10-21' 'BAL' 'TAM']\n",
      "2024-10-21: w_odds:1.58 acct_val: 729.17 usable cash: 656.26 won: True\n",
      "2024-10-21: w_odds:1.94 acct_val: 771.65 usable cash: 579.09 won: False\n",
      "2024-10-21: w_odds:1.12 acct_val: 694.48 usable cash: 509.64 won: False\n",
      "2024-10-21: w_odds:1.58 acct_val: 625.03 usable cash: 447.14 won: False\n",
      "2024-10-21: w_odds:1.58 acct_val: 562.53 usable cash: 390.89 won: False\n",
      "2024-10-21: w_odds:1.16 acct_val: 506.28 usable cash: 340.26 won: False\n",
      "2024-10-21: w_odds:1.36 acct_val: 455.65 usable cash: 294.69 won: False\n",
      "2024-10-21: w_odds:2.07 acct_val: 410.08 usable cash: 253.69 won: True\n",
      "2024-10-21: w_odds:1.25 acct_val: 454.10 usable cash: 208.27 won: False\n",
      "2024-10-21: w_odds:2.07 acct_val: 408.69 usable cash: 167.41 won: False\n",
      "['2024-10-27' 'CAR' 'DEN']\n",
      "2024-10-27: w_odds:1.43 acct_val: 367.82 usable cash: 331.04 won: True\n",
      "2024-10-27: w_odds:1.94 acct_val: 383.59 usable cash: 292.68 won: False\n",
      "['2024-10-28' 'NYG' 'PIT']\n",
      "2024-10-28: w_odds:1.05 acct_val: 345.23 usable cash: 313.84 won: False\n",
      "2024-10-28: w_odds:1.79 acct_val: 313.84 usable cash: 285.31 won: False\n",
      "2024-10-28: w_odds:1.06 acct_val: 285.31 usable cash: 259.38 won: False\n",
      "2024-10-28: w_odds:1.36 acct_val: 259.38 usable cash: 235.80 won: False\n",
      "2024-10-28: w_odds:1.79 acct_val: 235.80 usable cash: 214.36 won: False\n",
      "2024-10-28: w_odds:1.17 acct_val: 214.36 usable cash: 194.87 won: True\n",
      "2024-10-28: w_odds:1.58 acct_val: 217.71 usable cash: 175.08 won: True\n",
      "2024-10-28: w_odds:2.07 acct_val: 229.24 usable cash: 154.24 won: True\n",
      "2024-10-28: w_odds:1.40 acct_val: 251.61 usable cash: 131.37 won: True\n",
      "2024-10-28: w_odds:1.46 acct_val: 260.69 usable cash: 107.67 won: True\n",
      "2024-10-28: w_odds:1.25 acct_val: 271.63 usable cash: 82.97 won: False\n",
      "['2024-10-31' 'HOU' 'NYJ']\n",
      "2024-10-31: w_odds:1.33 acct_val: 246.94 usable cash: 222.24 won: False\n",
      "['2024-11-03' 'LAC' 'CLE']\n",
      "2024-11-03: w_odds:1.72 acct_val: 222.24 usable cash: 200.02 won: True\n",
      "['2024-11-04' 'TAM' 'KAN']\n",
      "2024-11-04: w_odds:1.76 acct_val: 238.35 usable cash: 214.52 won: True\n",
      "2024-11-04: w_odds:1.72 acct_val: 256.40 usable cash: 188.88 won: True\n",
      "2024-11-04: w_odds:1.58 acct_val: 274.98 usable cash: 161.38 won: False\n",
      "2024-11-04: w_odds:1.43 acct_val: 247.48 usable cash: 136.63 won: True\n",
      "2024-11-04: w_odds:1.33 acct_val: 258.09 usable cash: 110.82 won: False\n",
      "2024-11-04: w_odds:1.20 acct_val: 232.28 usable cash: 87.60 won: False\n",
      "2024-11-04: w_odds:1.16 acct_val: 209.05 usable cash: 66.69 won: False\n",
      "2024-11-04: w_odds:1.33 acct_val: 188.15 usable cash: 47.88 won: False\n",
      "2024-11-04: w_odds:1.20 acct_val: 169.33 usable cash: 30.94 won: False\n",
      "2024-11-04: w_odds:1.79 acct_val: 152.40 usable cash: 15.70 won: False\n",
      "['2024-11-07' 'CIN' 'BAL']\n",
      "2024-11-07: w_odds:1.16 acct_val: 137.16 usable cash: 123.44 won: False\n",
      "['2024-11-10' 'TEN' 'LAC']\n",
      "2024-11-10: w_odds:1.33 acct_val: 123.44 usable cash: 111.10 won: False\n",
      "['2024-11-14' 'WAS' 'PHI']\n",
      "2024-11-14: w_odds:1.19 acct_val: 111.10 usable cash: 99.99 won: False\n",
      "2024-11-14: w_odds:1.25 acct_val: 99.99 usable cash: 89.99 won: True\n",
      "2024-11-14: w_odds:1.43 acct_val: 102.49 usable cash: 79.74 won: True\n",
      "2024-11-14: w_odds:1.40 acct_val: 106.88 usable cash: 69.05 won: True\n",
      "2024-11-14: w_odds:1.25 acct_val: 111.12 usable cash: 57.94 won: False\n",
      "2024-11-14: w_odds:2.03 acct_val: 100.01 usable cash: 47.94 won: True\n",
      "2024-11-14: w_odds:1.25 acct_val: 110.30 usable cash: 36.91 won: True\n",
      "2024-11-14: w_odds:1.30 acct_val: 113.05 usable cash: 25.60 won: True\n",
      "['2024-11-17' 'LVR' 'MIA']\n",
      "2024-11-17: w_odds:1.43 acct_val: 116.43 usable cash: 104.79 won: False\n",
      "['2024-11-18' 'HOU' 'DAL']\n",
      "2024-11-18: w_odds:1.20 acct_val: 104.79 usable cash: 94.31 won: False\n",
      "2024-11-18: w_odds:1.04 acct_val: 94.31 usable cash: 84.88 won: False\n",
      "2024-11-18: w_odds:1.33 acct_val: 84.88 usable cash: 76.39 won: False\n",
      "2024-11-18: w_odds:1.72 acct_val: 76.39 usable cash: 68.75 won: False\n",
      "2024-11-18: w_odds:1.94 acct_val: 68.75 usable cash: 61.87 won: False\n",
      "2024-11-18: w_odds:1.33 acct_val: 61.87 usable cash: 55.69 won: True\n",
      "2024-11-18: w_odds:1.76 acct_val: 63.91 usable cash: 49.30 won: False\n",
      "2024-11-18: w_odds:1.83 acct_val: 57.52 usable cash: 43.54 won: False\n",
      "['2024-11-24' 'NWE' 'MIA']\n",
      "2024-11-24: w_odds:1.25 acct_val: 51.77 usable cash: 46.59 won: True\n",
      "['2024-11-25' 'BAL' 'LAC']\n",
      "2024-11-25: w_odds:1.20 acct_val: 53.06 usable cash: 47.76 won: False\n",
      "2024-11-25: w_odds:1.33 acct_val: 47.76 usable cash: 42.98 won: True\n",
      "2024-11-25: w_odds:1.33 acct_val: 49.33 usable cash: 38.05 won: False\n",
      "2024-11-25: w_odds:1.94 acct_val: 44.40 usable cash: 33.61 won: False\n",
      "2024-11-25: w_odds:1.36 acct_val: 39.96 usable cash: 29.61 won: True\n",
      "2024-11-25: w_odds:1.25 acct_val: 41.41 usable cash: 25.47 won: True\n",
      "2024-11-25: w_odds:1.09 acct_val: 42.44 usable cash: 21.23 won: True\n",
      "2024-11-25: w_odds:1.58 acct_val: 42.84 usable cash: 16.94 won: True\n",
      "2024-11-25: w_odds:1.58 acct_val: 45.33 usable cash: 12.41 won: True\n",
      "['2024-11-28' 'NYG' 'DAL']\n",
      "2024-11-28: w_odds:1.58 acct_val: 47.97 usable cash: 43.18 won: True\n",
      "['2024-11-29' 'LVR' 'KAN']\n",
      "2024-11-29: w_odds:1.43 acct_val: 50.77 usable cash: 45.69 won: False\n",
      "2024-11-29: w_odds:1.46 acct_val: 45.69 usable cash: 41.12 won: False\n",
      "2024-11-29: w_odds:1.12 acct_val: 41.12 usable cash: 37.01 won: False\n",
      "['2024-12-01' 'IND' 'NWE']\n",
      "2024-12-01: w_odds:1.04 acct_val: 37.01 usable cash: 33.31 won: False\n",
      "['2024-12-02' 'CLE' 'DEN']\n",
      "2024-12-02: w_odds:1.72 acct_val: 33.31 usable cash: 29.98 won: False\n",
      "2024-12-02: w_odds:1.58 acct_val: 29.98 usable cash: 26.98 won: True\n",
      "2024-12-02: w_odds:1.30 acct_val: 31.72 usable cash: 23.81 won: True\n",
      "2024-12-02: w_odds:1.99 acct_val: 32.67 usable cash: 20.54 won: False\n",
      "2024-12-02: w_odds:1.46 acct_val: 29.40 usable cash: 17.60 won: False\n",
      "2024-12-02: w_odds:1.33 acct_val: 26.46 usable cash: 14.95 won: False\n",
      "2024-12-02: w_odds:1.83 acct_val: 23.82 usable cash: 12.57 won: False\n",
      "2024-12-02: w_odds:1.30 acct_val: 21.43 usable cash: 10.43 won: False\n",
      "['2024-12-05' 'GNB' 'DET']\n",
      "2024-12-05: w_odds:1.30 acct_val: 19.29 usable cash: 17.36 won: False\n",
      "['2024-12-08' 'CLE' 'PIT']\n",
      "2024-12-08: w_odds:1.46 acct_val: 17.36 usable cash: 15.63 won: False\n",
      "['2024-12-09' 'CIN' 'DAL']\n",
      "2024-12-09: w_odds:1.33 acct_val: 15.63 usable cash: 14.06 won: False\n",
      "2024-12-09: w_odds:1.30 acct_val: 14.06 usable cash: 12.66 won: False\n",
      "2024-12-09: w_odds:1.40 acct_val: 12.66 usable cash: 11.39 won: False\n",
      "2024-12-09: w_odds:1.58 acct_val: 11.39 usable cash: 10.25 won: False\n",
      "2024-12-09: w_odds:1.30 acct_val: 10.25 usable cash: 9.23 won: False\n",
      "2024-12-09: w_odds:1.36 acct_val: 9.23 usable cash: 8.30 won: True\n",
      "2024-12-09: w_odds:1.02 acct_val: 9.56 usable cash: 7.35 won: False\n",
      "2024-12-09: w_odds:1.33 acct_val: 8.61 usable cash: 6.49 won: False\n",
      "['2024-12-15' 'NWE' 'ARI']\n",
      "2024-12-15: w_odds:1.40 acct_val: 7.74 usable cash: 6.97 won: True\n",
      "['2024-12-16' 'ATL' 'LVR']\n",
      "2024-12-16: w_odds:1.33 acct_val: 8.05 usable cash: 7.38 won: False\n",
      "2024-12-16: w_odds:1.72 acct_val: 7.38 usable cash: 6.77 won: False\n",
      "2024-12-16: w_odds:1.20 acct_val: 6.77 usable cash: 6.20 won: True\n",
      "2024-12-16: w_odds:1.36 acct_val: 6.88 usable cash: 5.63 won: False\n",
      "2024-12-16: w_odds:2.07 acct_val: 6.31 usable cash: 5.10 won: True\n",
      "2024-12-16: w_odds:0.96 acct_val: 6.87 usable cash: 4.53 won: True\n",
      "2024-12-16: w_odds:2.03 acct_val: 6.85 usable cash: 3.96 won: False\n",
      "2024-12-16: w_odds:1.43 acct_val: 6.28 usable cash: 3.44 won: True\n",
      "2024-12-16: w_odds:1.58 acct_val: 6.50 usable cash: 2.89 won: True\n",
      "2024-12-16: w_odds:1.46 acct_val: 6.82 usable cash: 2.33 won: True\n",
      "2024-12-16: w_odds:1.40 acct_val: 7.08 usable cash: 1.74 won: False\n",
      "2024-12-16: w_odds:1.33 acct_val: 6.49 usable cash: 1.20 won: False\n",
      "['2024-12-19' 'DEN' 'LAC']\n",
      "2024-12-19: w_odds:1.33 acct_val: 5.95 usable cash: 5.35 won: True\n",
      "2024-12-19: w_odds:1.25 acct_val: 6.14 usable cash: 4.74 won: False\n",
      "['2024-12-21' 'PIT' 'BAL']\n",
      "2024-12-21: w_odds:1.72 acct_val: 5.53 usable cash: 4.98 won: False\n",
      "['2024-12-22' 'JAX' 'LVR']\n",
      "2024-12-22: w_odds:1.25 acct_val: 4.98 usable cash: 4.48 won: False\n",
      "2024-12-22: w_odds:1.46 acct_val: 4.48 usable cash: 4.03 won: False\n",
      "['2024-12-23' 'NOR' 'GNB']\n",
      "2024-12-23: w_odds:1.72 acct_val: 4.03 usable cash: 3.63 won: False\n",
      "2024-12-23: w_odds:1.43 acct_val: 3.63 usable cash: 3.27 won: False\n",
      "2024-12-23: w_odds:1.02 acct_val: 3.27 usable cash: 2.94 won: False\n",
      "2024-12-23: w_odds:1.12 acct_val: 2.94 usable cash: 2.64 won: False\n",
      "2024-12-23: w_odds:1.99 acct_val: 2.64 usable cash: 2.38 won: False\n",
      "2024-12-23: w_odds:1.25 acct_val: 2.38 usable cash: 2.14 won: True\n",
      "2024-12-23: w_odds:1.16 acct_val: 2.44 usable cash: 1.90 won: False\n",
      "2024-12-23: w_odds:1.72 acct_val: 2.20 usable cash: 1.68 won: False\n",
      "['2024-12-25' 'BAL' 'HOU']\n",
      "2024-12-25: w_odds:1.02 acct_val: 1.98 usable cash: 1.78 won: False\n",
      "['2024-12-26' 'SEA' 'CHI']\n",
      "2024-12-26: w_odds:1.30 acct_val: 1.78 usable cash: 1.60 won: True\n",
      "2024-12-26: w_odds:1.76 acct_val: 1.83 usable cash: 1.42 won: False\n",
      "['2024-12-28' 'DEN' 'CIN']\n",
      "2024-12-28: w_odds:1.40 acct_val: 1.65 usable cash: 1.48 won: False\n",
      "['2024-12-29' 'GNB' 'MIN']\n",
      "2024-12-29: w_odds:1.58 acct_val: 1.48 usable cash: 1.34 won: False\n",
      "2024-12-29: w_odds:1.33 acct_val: 1.34 usable cash: 1.20 won: False\n",
      "['2024-12-30' 'DET' 'SFO']\n",
      "2024-12-30: w_odds:1.83 acct_val: 1.20 usable cash: 1.08 won: False\n",
      "2024-12-30: w_odds:1.20 acct_val: 1.08 usable cash: 0.97 won: False\n",
      "2024-12-30: w_odds:1.79 acct_val: 0.97 usable cash: 0.88 won: True\n",
      "2024-12-30: w_odds:1.83 acct_val: 1.05 usable cash: 0.77 won: False\n",
      "2024-12-30: w_odds:1.58 acct_val: 0.95 usable cash: 0.68 won: True\n",
      "2024-12-30: w_odds:1.46 acct_val: 1.00 usable cash: 0.58 won: False\n",
      "2024-12-30: w_odds:1.12 acct_val: 0.90 usable cash: 0.49 won: False\n",
      "['2025-01-04' 'CLE' 'BAL']\n",
      "2025-01-04: w_odds:1.43 acct_val: 0.81 usable cash: 0.73 won: False\n",
      "['2025-01-05' 'WAS' 'DAL']\n",
      "2025-01-05: w_odds:0.96 acct_val: 0.73 usable cash: 0.66 won: False\n",
      "['2025-01-11' 'LAC' 'HOU']\n",
      "2025-01-11: w_odds:1.25 acct_val: 0.66 usable cash: 0.60 won: True\n",
      "2025-01-11: w_odds:1.08 acct_val: 0.67 usable cash: 0.54 won: False\n",
      "2025-01-11: w_odds:1.58 acct_val: 0.61 usable cash: 0.48 won: False\n",
      "2025-01-11: w_odds:1.58 acct_val: 0.55 usable cash: 0.43 won: False\n",
      "2025-01-11: w_odds:1.38 acct_val: 0.50 usable cash: 0.38 won: False\n",
      "2025-01-11: w_odds:1.25 acct_val: 0.46 usable cash: 0.34 won: True\n",
      "2025-01-11: w_odds:0.99 acct_val: 0.47 usable cash: 0.30 won: False\n",
      "2025-01-11: w_odds:1.76 acct_val: 0.43 usable cash: 0.26 won: True\n",
      "2025-01-11: w_odds:1.72 acct_val: 0.46 usable cash: 0.22 won: False\n",
      "2025-01-11: w_odds:1.46 acct_val: 0.41 usable cash: 0.18 won: False\n",
      "2025-01-11: w_odds:1.20 acct_val: 0.38 usable cash: 0.15 won: True\n",
      "['2025-01-12' 'DEN' 'BUF']\n",
      "2025-01-12: w_odds:2.07 acct_val: 0.38 usable cash: 0.35 won: False\n",
      "['2025-01-13' 'MIN' 'LAR']\n",
      "2025-01-13: w_odds:1.20 acct_val: 0.35 usable cash: 0.31 won: False\n",
      "2025-01-13: w_odds:1.36 acct_val: 0.31 usable cash: 0.28 won: False\n",
      "['2025-01-18' 'HOU' 'KAN']\n",
      "2025-01-18: w_odds:2.07 acct_val: 0.28 usable cash: 0.25 won: False\n",
      "['2025-01-19' 'LAR' 'PHI']\n",
      "2025-01-19: w_odds:1.16 acct_val: 0.25 usable cash: 0.23 won: False\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'final_value': 0.22647537966608258,\n",
       " 'roi': -0.9997735246203339,\n",
       " 'win_rate': 0.32679738562091504,\n",
       " 'max_drawdown': 0.9997735246203339,\n",
       " 'total_bets': 153}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_utils.backtest_model(model, perf_conts, perf_y_col, perf_date_col, initial_capital=1000, position_size=0.1, \n",
    "                   confidence_threshold=0.0, show_plot=True, max_won_odds=2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109e37c",
   "metadata": {},
   "source": [
    "Value:  0.37491718013436764\n",
    "\n",
    "Params: \n",
    "    first_layer_size: 8\n",
    "    num_layers: 2\n",
    "    n_epochs: 39\n",
    "    dropout: 0.47875406200808335\n",
    "    lr: 0.009997751942238913\n",
    "    \n",
    "\n",
    "\n",
    "Value:  0.3759073484440955\n",
    "Params: \n",
    "    first_layer_size: 8\n",
    "    num_layers: 2\n",
    "    n_epochs: 68\n",
    "    dropout: 0.4497689844977892\n",
    "    lr: 0.007977206154472633\n",
    "    \n",
    "    \n",
    "    \n",
    "12/6\n",
    "\n",
    "Trial 206 finished with value: 0.547218605316966 and parameters: {'criterion': 'MSELoss', 'first_layer_size': 32, 'num_layers': 5, 'confidence_threshold': 0.05966702820817666, 'n_epochs': 379, 'dropout': 0.36961850006275193, 'lr': 0.008649806179332952}. Best is trial 206 with value: 0.547218605316966.\n",
    "\n",
    "[I 2024-12-06 12:49:28,047] Trial 579 finished with value: 0.5335308702482566 and parameters: {'criterion': 'SmoothL1Loss', 'first_layer_size': 16, 'num_layers': 3, 'confidence_threshold': 0.059979796814548306, 'n_epochs': 481, 'dropout': 0.2511747953677191, 'lr': 0.007942836869449217}. Best is trial 579 with value: 0.5335308702482566.\n",
    "\n",
    "\n",
    "[I 2024-12-06 14:53:00,850] Trial 385 finished with value: 0.5547767877242975 and parameters: {'criterion': 'MSELoss', 'first_layer_size': 56, 'num_layers': 6, 'confidence_threshold': 0.003263372268063613, 'n_epochs': 300, 'dropout': 0.3153661030384182, 'lr': 0.00593138298730814}. Best is trial 385 with value: 0.5547767877242975.\n",
    "\n",
    "[I 2024-12-06 15:32:38,969] Trial 583 finished with value: 0.550872165273167 and parameters: {'criterion': 'MSELoss', 'first_layer_size': 56, 'num_layers': 7, 'confidence_threshold': 0.010458110701511005, 'n_epochs': 336, 'dropout': 0.3188974735143638, 'lr': 0.006976522077116529}. Best is trial 385 with value: 0.5547767877242975.\n",
    "\n",
    "[I 2024-12-06 15:55:26,133] Trial 669 finished with value: 0.5645423555160363 and parameters: {'criterion': 'MSELoss', 'first_layer_size': 56, 'num_layers': 7, 'confidence_threshold': 0.0243907527056759, 'n_epochs': 370, 'dropout': 0.32998724447261185, 'lr': 0.0075018456671685696}. Best is trial 669 with value: 0.5645423555160363.\n",
    "\n",
    "[I 2024-12-06 20:33:46,555] Trial 1737 finished with value: 0.5716002919237433 and parameters: {'criterion': 'MSELoss', 'first_layer_size': 56, 'num_layers': 7, 'confidence_threshold': 0.015368961704784596, 'n_epochs': 328, 'dropout': 0.348927921024466, 'lr': 0.009575624984802092}. Best is trial 1737 with value: 0.5716002919237433.\n",
    "\n",
    "\n",
    "[I 2024-12-06 21:17:06,545] Trial 1889 finished with value: 0.5739803740995499 and parameters: {'criterion': 'MSELoss', 'first_layer_size': 56, 'num_layers': 7, 'confidence_threshold': 0.014029751567812504, 'n_epochs': 357, 'dropout': 0.34275064196127053, 'lr': 0.008692336113071646}. Best is trial 1889 with value: 0.5739803740995499.\n",
    "\n",
    "[I 2024-12-10 09:39:54,796] Trial 1612 finished with value: 0.5748324966932515 and parameters: {'criterion': 'MSELoss', 'first_layer_size': 64, 'num_layers': 7, 'confidence_threshold': 0.003855147984840053, 'dropout': 0.3182765851196762, 'lr': 0.008210651343970551, 'n_epochs': 219}. Best is trial 1612 with value: 0.5748324966932515.\n",
    "\n",
    "\n",
    "\n",
    "Trial 1749 finished with value: 0.5772962775717783 and parameters: {'criterion': 'MSELoss', 'first_layer_size': 64, 'num_layers': 7, 'confidence_threshold': 1.8653941637231173e-05, 'dropout': 0.3181431308672629, 'lr': 0.009969598746996452, 'n_epochs': 231}.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb328981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2024-02-11' 'SFO' 'KAN']\n",
      "2024-02-11: w_odds:2.89 acct_val: 1000.00 usable cash: 900.00 won: False\n",
      "['2024-10-10' 'SFO' 'SEA']\n",
      "2024-10-10: w_odds:2.03 acct_val: 900.00 usable cash: 810.00 won: False\n",
      "['2024-10-13' 'JAX' 'CHI']\n",
      "2024-10-13: w_odds:1.46 acct_val: 810.00 usable cash: 729.00 won: False\n",
      "['2024-10-14' 'BUF' 'NYJ']\n",
      "2024-10-14: w_odds:1.99 acct_val: 729.00 usable cash: 656.10 won: False\n",
      "2024-10-14: w_odds:1.36 acct_val: 656.10 usable cash: 590.49 won: False\n",
      "2024-10-14: w_odds:1.43 acct_val: 590.49 usable cash: 531.44 won: True\n",
      "2024-10-14: w_odds:1.58 acct_val: 615.80 usable cash: 469.86 won: True\n",
      "2024-10-14: w_odds:1.46 acct_val: 651.67 usable cash: 404.69 won: False\n",
      "2024-10-14: w_odds:1.30 acct_val: 586.50 usable cash: 346.04 won: True\n",
      "2024-10-14: w_odds:1.25 acct_val: 604.00 usable cash: 285.64 won: False\n",
      "2024-10-14: w_odds:1.33 acct_val: 543.60 usable cash: 231.29 won: True\n",
      "['2024-10-17' 'DEN' 'NOR']\n",
      "2024-10-17: w_odds:1.83 acct_val: 561.51 usable cash: 505.36 won: False\n",
      "['2024-10-20' 'PHI' 'NYG']\n",
      "2024-10-20: w_odds:1.58 acct_val: 505.36 usable cash: 454.83 won: False\n",
      "['2024-10-21' 'BAL' 'TAM']\n",
      "2024-10-21: w_odds:1.58 acct_val: 454.83 usable cash: 413.48 won: True\n",
      "2024-10-21: w_odds:1.94 acct_val: 478.91 usable cash: 369.94 won: False\n",
      "2024-10-21: w_odds:1.12 acct_val: 435.37 usable cash: 330.36 won: False\n",
      "2024-10-21: w_odds:2.32 acct_val: 395.79 usable cash: 294.38 won: True\n",
      "2024-10-21: w_odds:1.58 acct_val: 443.42 usable cash: 254.07 won: False\n",
      "2024-10-21: w_odds:1.58 acct_val: 403.11 usable cash: 217.42 won: False\n",
      "2024-10-21: w_odds:1.16 acct_val: 366.46 usable cash: 184.11 won: False\n",
      "2024-10-21: w_odds:1.36 acct_val: 333.15 usable cash: 153.82 won: True\n",
      "2024-10-21: w_odds:2.07 acct_val: 344.12 usable cash: 122.54 won: True\n",
      "2024-10-21: w_odds:1.25 acct_val: 377.70 usable cash: 88.20 won: False\n",
      "2024-10-21: w_odds:2.07 acct_val: 343.36 usable cash: 56.99 won: True\n",
      "['2024-10-24' 'MIN' 'LAR']\n",
      "2024-10-24: w_odds:1.43 acct_val: 376.87 usable cash: 339.18 won: True\n",
      "2024-10-24: w_odds:1.94 acct_val: 393.02 usable cash: 299.88 won: False\n",
      "['2024-10-27' 'CAR' 'DEN']\n",
      "2024-10-27: w_odds:2.32 acct_val: 353.72 usable cash: 318.35 won: False\n",
      "['2024-10-28' 'NYG' 'PIT']\n",
      "2024-10-28: w_odds:1.05 acct_val: 318.35 usable cash: 295.61 won: False\n",
      "2024-10-28: w_odds:1.79 acct_val: 295.61 usable cash: 274.49 won: False\n",
      "2024-10-28: w_odds:1.06 acct_val: 274.49 usable cash: 254.89 won: False\n",
      "2024-10-28: w_odds:1.36 acct_val: 254.89 usable cash: 236.68 won: False\n",
      "2024-10-28: w_odds:1.79 acct_val: 236.68 usable cash: 219.78 won: False\n",
      "2024-10-28: w_odds:4.31 acct_val: 219.78 usable cash: 204.08 won: False\n",
      "2024-10-28: w_odds:1.17 acct_val: 204.08 usable cash: 189.50 won: True\n",
      "2024-10-28: w_odds:1.58 acct_val: 206.59 usable cash: 174.74 won: True\n",
      "2024-10-28: w_odds:2.07 acct_val: 215.18 usable cash: 159.37 won: True\n",
      "2024-10-28: w_odds:2.76 acct_val: 231.68 usable cash: 142.83 won: False\n",
      "2024-10-28: w_odds:1.40 acct_val: 215.13 usable cash: 127.46 won: False\n",
      "2024-10-28: w_odds:3.80 acct_val: 199.76 usable cash: 113.19 won: False\n",
      "2024-10-28: w_odds:1.46 acct_val: 185.49 usable cash: 99.94 won: True\n",
      "2024-10-28: w_odds:1.25 acct_val: 191.61 usable cash: 86.25 won: False\n",
      "['2024-10-31' 'HOU' 'NYJ']\n",
      "2024-10-31: w_odds:1.33 acct_val: 177.93 usable cash: 160.14 won: False\n",
      "['2024-11-03' 'NOR' 'CAR']\n",
      "2024-11-03: w_odds:1.72 acct_val: 160.14 usable cash: 144.12 won: False\n",
      "['2024-11-04' 'TAM' 'KAN']\n",
      "2024-11-04: w_odds:3.80 acct_val: 144.12 usable cash: 131.02 won: False\n",
      "2024-11-04: w_odds:1.76 acct_val: 131.02 usable cash: 119.11 won: True\n",
      "2024-11-04: w_odds:1.72 acct_val: 140.04 usable cash: 106.38 won: False\n",
      "2024-11-04: w_odds:1.58 acct_val: 127.31 usable cash: 94.80 won: False\n",
      "2024-11-04: w_odds:1.43 acct_val: 115.73 usable cash: 84.28 won: True\n",
      "2024-11-04: w_odds:1.33 acct_val: 120.24 usable cash: 73.35 won: False\n",
      "2024-11-04: w_odds:1.20 acct_val: 109.31 usable cash: 63.42 won: False\n",
      "2024-11-04: w_odds:1.16 acct_val: 99.37 usable cash: 54.38 won: False\n",
      "2024-11-04: w_odds:1.33 acct_val: 90.34 usable cash: 46.17 won: False\n",
      "2024-11-04: w_odds:1.20 acct_val: 82.13 usable cash: 38.70 won: False\n",
      "2024-11-04: w_odds:1.79 acct_val: 74.66 usable cash: 31.92 won: False\n",
      "['2024-11-07' 'CIN' 'BAL']\n",
      "2024-11-07: w_odds:1.16 acct_val: 67.87 usable cash: 61.09 won: False\n",
      "['2024-11-10' 'TEN' 'LAC']\n",
      "2024-11-10: w_odds:1.33 acct_val: 61.09 usable cash: 54.98 won: False\n",
      "['2024-11-14' 'WAS' 'PHI']\n",
      "2024-11-14: w_odds:1.19 acct_val: 54.98 usable cash: 49.98 won: False\n",
      "2024-11-14: w_odds:2.64 acct_val: 49.98 usable cash: 45.44 won: False\n",
      "2024-11-14: w_odds:3.22 acct_val: 45.44 usable cash: 41.31 won: False\n",
      "2024-11-14: w_odds:1.25 acct_val: 41.31 usable cash: 37.55 won: True\n",
      "2024-11-14: w_odds:1.43 acct_val: 42.24 usable cash: 33.71 won: True\n",
      "2024-11-14: w_odds:1.40 acct_val: 43.89 usable cash: 29.72 won: True\n",
      "2024-11-14: w_odds:3.42 acct_val: 45.47 usable cash: 25.59 won: True\n",
      "2024-11-14: w_odds:1.25 acct_val: 55.47 usable cash: 20.54 won: False\n",
      "2024-11-14: w_odds:2.03 acct_val: 50.43 usable cash: 15.96 won: False\n",
      "2024-11-14: w_odds:1.25 acct_val: 45.84 usable cash: 11.79 won: True\n",
      "2024-11-14: w_odds:1.30 acct_val: 46.88 usable cash: 7.53 won: False\n",
      "['2024-11-17' 'LVR' 'MIA']\n",
      "2024-11-17: w_odds:1.43 acct_val: 42.62 usable cash: 38.36 won: False\n",
      "['2024-11-18' 'HOU' 'DAL']\n",
      "2024-11-18: w_odds:1.20 acct_val: 38.36 usable cash: 34.87 won: False\n",
      "2024-11-18: w_odds:1.04 acct_val: 34.87 usable cash: 31.70 won: False\n",
      "2024-11-18: w_odds:1.33 acct_val: 31.70 usable cash: 28.82 won: True\n",
      "2024-11-18: w_odds:1.72 acct_val: 32.65 usable cash: 25.85 won: False\n",
      "2024-11-18: w_odds:1.94 acct_val: 29.68 usable cash: 23.15 won: False\n",
      "2024-11-18: w_odds:2.32 acct_val: 26.99 usable cash: 20.70 won: False\n",
      "2024-11-18: w_odds:2.76 acct_val: 24.53 usable cash: 18.47 won: False\n",
      "2024-11-18: w_odds:1.33 acct_val: 22.30 usable cash: 16.44 won: True\n",
      "2024-11-18: w_odds:1.76 acct_val: 22.97 usable cash: 14.35 won: False\n",
      "2024-11-18: w_odds:3.22 acct_val: 20.88 usable cash: 12.46 won: False\n",
      "2024-11-18: w_odds:1.83 acct_val: 18.98 usable cash: 10.73 won: False\n",
      "['2024-11-21' 'PIT' 'CLE']\n",
      "2024-11-21: w_odds:1.25 acct_val: 17.26 usable cash: 15.53 won: True\n",
      "['2024-11-24' 'NWE' 'MIA']\n",
      "2024-11-24: w_odds:2.64 acct_val: 17.69 usable cash: 15.92 won: False\n",
      "['2024-11-25' 'BAL' 'LAC']\n",
      "2024-11-25: w_odds:1.20 acct_val: 15.92 usable cash: 14.47 won: False\n",
      "2024-11-25: w_odds:6.74 acct_val: 14.47 usable cash: 13.16 won: False\n",
      "2024-11-25: w_odds:1.33 acct_val: 13.16 usable cash: 11.96 won: True\n",
      "2024-11-25: w_odds:1.33 acct_val: 13.55 usable cash: 10.73 won: False\n",
      "2024-11-25: w_odds:1.94 acct_val: 12.32 usable cash: 9.61 won: False\n",
      "2024-11-25: w_odds:1.36 acct_val: 11.20 usable cash: 8.59 won: True\n",
      "2024-11-25: w_odds:1.25 acct_val: 11.57 usable cash: 7.54 won: True\n",
      "2024-11-25: w_odds:1.09 acct_val: 11.83 usable cash: 6.46 won: True\n",
      "2024-11-25: w_odds:4.51 acct_val: 11.93 usable cash: 5.38 won: False\n",
      "2024-11-25: w_odds:1.58 acct_val: 10.85 usable cash: 4.39 won: True\n",
      "2024-11-25: w_odds:1.58 acct_val: 11.42 usable cash: 3.35 won: True\n",
      "['2024-11-28' 'NYG' 'DAL']\n",
      "2024-11-28: w_odds:1.58 acct_val: 12.03 usable cash: 10.82 won: False\n",
      "['2024-11-29' 'LVR' 'KAN']\n",
      "2024-11-29: w_odds:1.43 acct_val: 10.82 usable cash: 9.74 won: False\n",
      "2024-11-29: w_odds:1.46 acct_val: 9.74 usable cash: 8.77 won: False\n",
      "2024-11-29: w_odds:1.12 acct_val: 8.77 usable cash: 7.89 won: False\n",
      "['2024-12-01' 'IND' 'NWE']\n",
      "2024-12-01: w_odds:1.04 acct_val: 7.89 usable cash: 7.10 won: False\n",
      "['2024-12-02' 'CLE' 'DEN']\n",
      "2024-12-02: w_odds:1.72 acct_val: 7.10 usable cash: 6.39 won: True\n",
      "2024-12-02: w_odds:1.58 acct_val: 7.62 usable cash: 5.63 won: True\n",
      "2024-12-02: w_odds:1.30 acct_val: 8.06 usable cash: 4.82 won: True\n",
      "2024-12-02: w_odds:1.99 acct_val: 8.30 usable cash: 3.99 won: False\n",
      "2024-12-02: w_odds:1.46 acct_val: 7.47 usable cash: 3.25 won: False\n",
      "2024-12-02: w_odds:1.33 acct_val: 6.72 usable cash: 2.57 won: False\n",
      "2024-12-02: w_odds:1.83 acct_val: 6.05 usable cash: 1.97 won: False\n",
      "2024-12-02: w_odds:2.32 acct_val: 5.45 usable cash: 1.42 won: False\n",
      "2024-12-02: w_odds:1.30 acct_val: 4.90 usable cash: 0.93 won: False\n",
      "2024-12-02: w_odds:2.32 acct_val: 4.41 usable cash: 0.49 won: False\n",
      "['2024-12-05' 'GNB' 'DET']\n",
      "2024-12-05: w_odds:1.30 acct_val: 3.97 usable cash: 3.57 won: False\n",
      "['2024-12-08' 'CLE' 'PIT']\n",
      "2024-12-08: w_odds:1.46 acct_val: 3.57 usable cash: 3.22 won: False\n",
      "['2024-12-09' 'CIN' 'DAL']\n",
      "2024-12-09: w_odds:1.33 acct_val: 3.22 usable cash: 2.89 won: False\n",
      "2024-12-09: w_odds:1.30 acct_val: 2.89 usable cash: 2.60 won: False\n",
      "2024-12-09: w_odds:2.64 acct_val: 2.60 usable cash: 2.34 won: False\n",
      "2024-12-09: w_odds:1.40 acct_val: 2.34 usable cash: 2.11 won: False\n",
      "2024-12-09: w_odds:1.58 acct_val: 2.11 usable cash: 1.90 won: False\n",
      "2024-12-09: w_odds:1.30 acct_val: 1.90 usable cash: 1.71 won: False\n",
      "2024-12-09: w_odds:2.32 acct_val: 1.71 usable cash: 1.54 won: False\n",
      "2024-12-09: w_odds:1.36 acct_val: 1.54 usable cash: 1.38 won: True\n",
      "2024-12-09: w_odds:1.02 acct_val: 1.59 usable cash: 1.22 won: False\n",
      "2024-12-09: w_odds:1.33 acct_val: 1.43 usable cash: 1.08 won: False\n",
      "['2024-12-12' 'LAR' 'SFO']\n",
      "2024-12-12: w_odds:1.40 acct_val: 1.29 usable cash: 1.16 won: True\n",
      "['2024-12-15' 'NWE' 'ARI']\n",
      "2024-12-15: w_odds:2.32 acct_val: 1.34 usable cash: 1.21 won: False\n",
      "['2024-12-16' 'ATL' 'LVR']\n",
      "2024-12-16: w_odds:1.33 acct_val: 1.21 usable cash: 1.11 won: False\n",
      "2024-12-16: w_odds:1.72 acct_val: 1.11 usable cash: 1.03 won: True\n",
      "2024-12-16: w_odds:1.20 acct_val: 1.18 usable cash: 0.94 won: True\n",
      "2024-12-16: w_odds:1.36 acct_val: 1.20 usable cash: 0.85 won: False\n",
      "2024-12-16: w_odds:2.07 acct_val: 1.10 usable cash: 0.76 won: False\n",
      "2024-12-16: w_odds:0.96 acct_val: 1.02 usable cash: 0.68 won: True\n",
      "2024-12-16: w_odds:2.03 acct_val: 1.02 usable cash: 0.61 won: False\n",
      "2024-12-16: w_odds:1.43 acct_val: 0.94 usable cash: 0.53 won: False\n",
      "2024-12-16: w_odds:1.58 acct_val: 0.87 usable cash: 0.47 won: True\n",
      "2024-12-16: w_odds:1.46 acct_val: 0.90 usable cash: 0.40 won: False\n",
      "2024-12-16: w_odds:1.40 acct_val: 0.83 usable cash: 0.33 won: False\n",
      "2024-12-16: w_odds:2.32 acct_val: 0.77 usable cash: 0.27 won: False\n",
      "2024-12-16: w_odds:1.33 acct_val: 0.71 usable cash: 0.22 won: True\n",
      "['2024-12-19' 'DEN' 'LAC']\n",
      "2024-12-19: w_odds:1.33 acct_val: 0.73 usable cash: 0.66 won: True\n",
      "2024-12-19: w_odds:1.25 acct_val: 0.75 usable cash: 0.58 won: False\n",
      "['2024-12-21' 'PIT' 'BAL']\n",
      "2024-12-21: w_odds:1.72 acct_val: 0.68 usable cash: 0.61 won: False\n",
      "['2024-12-22' 'JAX' 'LVR']\n",
      "2024-12-22: w_odds:1.25 acct_val: 0.61 usable cash: 0.55 won: False\n",
      "2024-12-22: w_odds:1.46 acct_val: 0.55 usable cash: 0.49 won: False\n",
      "['2024-12-23' 'NOR' 'GNB']\n",
      "2024-12-23: w_odds:1.72 acct_val: 0.49 usable cash: 0.45 won: False\n",
      "2024-12-23: w_odds:1.43 acct_val: 0.45 usable cash: 0.41 won: False\n",
      "2024-12-23: w_odds:3.04 acct_val: 0.41 usable cash: 0.37 won: False\n",
      "2024-12-23: w_odds:1.02 acct_val: 0.37 usable cash: 0.34 won: False\n",
      "2024-12-23: w_odds:1.12 acct_val: 0.34 usable cash: 0.31 won: False\n",
      "2024-12-23: w_odds:1.99 acct_val: 0.31 usable cash: 0.28 won: False\n",
      "2024-12-23: w_odds:1.25 acct_val: 0.28 usable cash: 0.25 won: True\n",
      "2024-12-23: w_odds:2.76 acct_val: 0.29 usable cash: 0.23 won: False\n",
      "2024-12-23: w_odds:2.89 acct_val: 0.26 usable cash: 0.20 won: False\n",
      "2024-12-23: w_odds:1.16 acct_val: 0.24 usable cash: 0.18 won: False\n",
      "2024-12-23: w_odds:1.72 acct_val: 0.21 usable cash: 0.16 won: True\n",
      "['2024-12-25' 'BAL' 'HOU']\n",
      "2024-12-25: w_odds:1.02 acct_val: 0.23 usable cash: 0.21 won: False\n",
      "['2024-12-26' 'SEA' 'CHI']\n",
      "2024-12-26: w_odds:1.30 acct_val: 0.21 usable cash: 0.19 won: True\n",
      "2024-12-26: w_odds:1.76 acct_val: 0.21 usable cash: 0.16 won: False\n",
      "['2024-12-28' 'DEN' 'CIN']\n",
      "2024-12-28: w_odds:1.40 acct_val: 0.19 usable cash: 0.17 won: False\n",
      "['2024-12-29' 'GNB' 'MIN']\n",
      "2024-12-29: w_odds:1.58 acct_val: 0.17 usable cash: 0.15 won: False\n",
      "2024-12-29: w_odds:1.33 acct_val: 0.15 usable cash: 0.14 won: False\n",
      "['2024-12-30' 'DET' 'SFO']\n",
      "2024-12-30: w_odds:1.83 acct_val: 0.14 usable cash: 0.13 won: False\n",
      "2024-12-30: w_odds:4.31 acct_val: 0.13 usable cash: 0.11 won: False\n",
      "2024-12-30: w_odds:1.20 acct_val: 0.11 usable cash: 0.10 won: False\n",
      "2024-12-30: w_odds:1.79 acct_val: 0.10 usable cash: 0.09 won: False\n",
      "2024-12-30: w_odds:1.83 acct_val: 0.09 usable cash: 0.08 won: False\n",
      "2024-12-30: w_odds:1.58 acct_val: 0.08 usable cash: 0.07 won: False\n",
      "2024-12-30: w_odds:1.46 acct_val: 0.07 usable cash: 0.07 won: False\n",
      "2024-12-30: w_odds:1.12 acct_val: 0.07 usable cash: 0.06 won: False\n",
      "['2025-01-04' 'CLE' 'BAL']\n",
      "2025-01-04: w_odds:1.43 acct_val: 0.06 usable cash: 0.05 won: True\n",
      "['2025-01-05' 'WAS' 'DAL']\n",
      "2025-01-05: w_odds:0.96 acct_val: 0.06 usable cash: 0.06 won: False\n",
      "['2025-01-11' 'LAC' 'HOU']\n",
      "2025-01-11: w_odds:1.25 acct_val: 0.06 usable cash: 0.05 won: True\n",
      "2025-01-11: w_odds:1.08 acct_val: 0.06 usable cash: 0.05 won: False\n",
      "2025-01-11: w_odds:5.75 acct_val: 0.05 usable cash: 0.04 won: False\n",
      "2025-01-11: w_odds:1.58 acct_val: 0.05 usable cash: 0.04 won: False\n",
      "2025-01-11: w_odds:1.58 acct_val: 0.05 usable cash: 0.04 won: False\n",
      "2025-01-11: w_odds:4.51 acct_val: 0.04 usable cash: 0.03 won: False\n",
      "2025-01-11: w_odds:2.64 acct_val: 0.04 usable cash: 0.03 won: False\n",
      "2025-01-11: w_odds:1.38 acct_val: 0.04 usable cash: 0.03 won: False\n",
      "2025-01-11: w_odds:1.25 acct_val: 0.03 usable cash: 0.03 won: True\n",
      "2025-01-11: w_odds:0.99 acct_val: 0.03 usable cash: 0.02 won: False\n",
      "2025-01-11: w_odds:1.76 acct_val: 0.03 usable cash: 0.02 won: False\n",
      "2025-01-11: w_odds:1.72 acct_val: 0.03 usable cash: 0.02 won: False\n",
      "2025-01-11: w_odds:1.46 acct_val: 0.03 usable cash: 0.02 won: False\n",
      "2025-01-11: w_odds:1.20 acct_val: 0.03 usable cash: 0.02 won: True\n",
      "['2025-01-12' 'DEN' 'BUF']\n",
      "2025-01-12: w_odds:2.07 acct_val: 0.03 usable cash: 0.02 won: True\n",
      "['2025-01-13' 'MIN' 'LAR']\n",
      "2025-01-13: w_odds:1.20 acct_val: 0.03 usable cash: 0.03 won: False\n",
      "2025-01-13: w_odds:1.36 acct_val: 0.03 usable cash: 0.02 won: False\n",
      "2025-01-13: w_odds:2.32 acct_val: 0.02 usable cash: 0.02 won: False\n",
      "['2025-01-18' 'HOU' 'KAN']\n",
      "2025-01-18: w_odds:2.07 acct_val: 0.02 usable cash: 0.02 won: False\n",
      "['2025-01-19' 'LAR' 'PHI']\n",
      "2025-01-19: w_odds:1.16 acct_val: 0.02 usable cash: 0.02 won: False\n",
      "2025-01-19: w_odds:4.51 acct_val: 0.02 usable cash: 0.02 won: False\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'final_value': 0.015363152411150683,\n",
       " 'roi': -0.9999846368475889,\n",
       " 'win_rate': 0.2620320855614973,\n",
       " 'max_drawdown': 0.9999846368475889,\n",
       " 'total_bets': 187}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_utils.backtest_model(pipeline, perf_conts, perf_y_col, perf_date_col, initial_capital=1000, position_size=0.05, confidence_threshold=best_params['confidence_threshold'], show_plot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b4e2bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25654450261780104\n"
     ]
    }
   ],
   "source": [
    "# Remove outside of confidence threshold\n",
    "mask = (probas < 0.5 - confidence_threshold) | (probas > 0.5 + confidence_threshold)\n",
    "predictions = np.where(mask, probas, np.nan)\n",
    "\n",
    "# Use numpy mask for nan values\n",
    "valid_mask = ~np.isnan(predictions)\n",
    "valid_predictions = predictions[valid_mask]\n",
    "valid_mask = valid_mask.flatten()\n",
    "perf_y_col_mask = perf_y_col[valid_mask]\n",
    "\n",
    "\n",
    "true_values = perf_y_col_mask[:,0].astype(np.int32)\n",
    "pred_values = valid_predictions.flatten()\n",
    "pred_values_int = np.rint(valid_predictions).flatten().astype(np.int32)\n",
    "\n",
    "model_win_prob = (1.0*(true_values == pred_values_int).sum()) / (true_values.shape[0])\n",
    "print(model_win_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d6ca8",
   "metadata": {},
   "source": [
    "# Using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de68871b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5264, 49)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c222c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8524f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "param_grid = {\n",
    "    'learning_rate': [ 0.008, 0.01, 0.03],           # Removed 0.5, 1 as they're often too aggressive\n",
    "    'max_depth': [3, 6, 9],                      # Simplified to 3 values, covering shallow to deep\n",
    "    'n_estimators': [100, 200],                  # Removed extremes, these are most common sweet spots\n",
    "    'subsample': [0.8, 1.0],                     # Removed 0.6 as it might be too aggressive for this dataset size\n",
    "    'colsample_bytree': [0.8, 1.0],              # Same as above\n",
    "    'min_child_weight': [1, 3],                  # Removed 5 as it might be too restrictive\n",
    "}\n",
    "aparam_grid = {\n",
    "    'learning_rate': [0.005, 0.01, 0.05],        # Removed 0.5, 1 as they're often too aggressive\n",
    "    'max_depth': [3],                         # Simplified to 3 values, covering shallow to deep\n",
    "    'n_estimators': [300, 350, 400],             # Removed extremes, these are most common sweet spots\n",
    "    'subsample': [0.5, 0.6, 0.7],                # Removed 0.6 as it might be too aggressive for this dataset size\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7],         # Same as above\n",
    "    'min_child_weight': [3, 4],                  # Removed 5 as it might be too restrictive\n",
    "}\n",
    "\n",
    "# model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model = XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Create a custom scorer using the F1 score\n",
    "# f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "# Tune hyperparameters using GridSearchCV with the custom F1 scorer\n",
    "# grid_search = GridSearchCV(model, param_grid, scoring=f1_scorer, cv=5, verbose=1)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5, verbose=1)\n",
    "grid_search.fit(conts_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acdae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee2ec9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# train final model w/ early stopping\n",
    "model = XGBClassifier(\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=10,\n",
    "    # **grid_search.best_params_,\n",
    "    # **{'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 6, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}\n",
    "    # 66.5%, dd 31.4%\n",
    "    # {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}\n",
    "    # 66.5%, dd 30.9%\n",
    "    # {'colsample_bytree': 1.0, 'learning_rate': 0.005, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 300, 'subsample': 0.6}\n",
    ")\n",
    "model = XGBRegressor(\n",
    "    objective='reg:squarederror', \n",
    "    # **grid_search.best_params_,\n",
    "    # 67.3 w/ kelly adjustments 0.2, 0.01\n",
    "    # {'colsample_bytree': 0.6, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 350, 'subsample': 0.6}\n",
    "    # 67.2, dd 28.68 kelly 0.25, 0.014\n",
    "    # **{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 350, 'subsample': 0.5}\n",
    "    \n",
    "   # **{'colsample_bytree': 0.6, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 300, 'subsample': 0.5}\n",
    "    **{'colsample_bytree': 1.0, 'learning_rate': 0.03, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
    "\n",
    "\n",
    ")\n",
    "model.fit(\n",
    "    conts_train,\n",
    "    y_train,\n",
    "    eval_set=[(conts_train, y_train)], # , (holdout_conts, holdout_y)\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_conts.shape\n",
    "perf_y_col.shape\n",
    "perf_y_col[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177675db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for test data\n",
    "# model = grid_search.best_estimator_\n",
    "y_pred = model.predict(perf_conts)\n",
    "\n",
    "nfl_utils.backtest_model(model, perf_conts, perf_y_col, perf_date_col, initial_capital=1000, position_size=0.1, \n",
    "                   confidence_threshold=0.0, show_plot=True, max_won_odds=2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb3d7cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make confusion matrix\n",
    "cm = confusion_matrix(perf_y_col[:,0], y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7025ece",
   "metadata": {},
   "source": [
    "## Save XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc9ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('xgboost_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e94a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6577d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8a1a349",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705bd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
